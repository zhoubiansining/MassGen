"""
å®Œæ•´æµ‹è¯•æµç¨‹ï¼šWriting Agent + Judge Agent

æ–°å¢ï¼šæ”¯æŒä»å‰åºæ£€ç´¢/åˆ†ææ­¥éª¤çš„ JSON è¾“å‡ºåŠ è½½ cluster_summariesï¼Œä¸ Retrieval/Analysis æ¨¡å—å¯¹é½ã€‚
"""

import argparse
import json
import os

from writing_agent import WritingAgent, ModelConfig
from judge_agent import JudgeAgent
from pipeline_adapter import analysis_to_cluster_summaries

# é…ç½®ï¼ˆä½¿ç”¨ Kimiï¼‰
CONFIG = ModelConfig(
    name="Kimi-K2",  # ä¿®æ­£ï¼šå»æ‰ $ å‰ç¼€
    api_key="sk-aRG9iu2Hy9--oPxrG-5faA",
    base_url="https://llmapi.paratera.com/v1/",
    temperature=0.5,
    max_tokens=4096
)

DEFAULT_TEST_DATA = {
    "cluster_1": {
        "topic": "Transformer æ¶æ„ä¸æ³¨æ„åŠ›æœºåˆ¶",
        "summary": "æœ¬ä¸»é¢˜æ¶µç›–äº† Transformer æ¶æ„çš„æå‡ºåŠå…¶æ³¨æ„åŠ›æœºåˆ¶çš„æ”¹è¿›æ–¹æ³•ã€‚",
        "papers": [
            {
                "paper_id": "paper_001",
                "title": "Attention is All You Need",
                "authors": ["Vaswani, A.", "Shazeer, N."],
                "year": 2017,
                "key_contribution": "æå‡ºäº†åŸå§‹ Transformer æ¶æ„ï¼ŒåŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶"
            },
            {
                "paper_id": "paper_002",
                "title": "BERT: Pre-training of Deep Bidirectional Transformers",
                "authors": ["Devlin, J.", "Chang, M. W."],
                "year": 2019,
                "key_contribution": "åŒå‘é¢„è®­ç»ƒæ–¹æ³•ï¼ŒMasked Language Model"
            },
            {
                "paper_id": "paper_003",
                "title": "GPT-3: Language Models are Few-Shot Learners",
                "authors": ["Brown, T.", "Mann, B."],
                "year": 2020,
                "key_contribution": "å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›"
            }
        ]
    }
}


def parse_args():
    parser = argparse.ArgumentParser(description="Run writing + judging pipeline with optional analysis JSON")
    parser.add_argument(
        "--analysis-json",
        dest="analysis_json",
        help="å‰åº Analysis è¾“å‡ºçš„ JSON è·¯å¾„ï¼ˆåŒ…å« clusters/insights/datasï¼‰"
    )
    return parser.parse_args()


def load_cluster_summaries(analysis_json: str = None):
    """è‹¥æä¾› analysis_jsonï¼Œåˆ™è½¬æ¢ä¸º WritingAgent æ‰€éœ€è¾“å…¥ï¼Œå¦åˆ™å›é€€åˆ°é»˜è®¤ç¤ºä¾‹æ•°æ®ã€‚"""
    if analysis_json:
        if not os.path.exists(analysis_json):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ° analysis_json æ–‡ä»¶: {analysis_json}")
        with open(analysis_json, "r", encoding="utf-8") as f:
            analysis_result = json.load(f)
        print(f"\nğŸ“¥ å·²ä» {analysis_json} è¯»å– Analysis ç»“æœï¼Œè½¬æ¢ä¸ºå†™ä½œè¾“å…¥æ ¼å¼...")
        return analysis_to_cluster_summaries(analysis_result)

    print("\nğŸ“¥ æœªæä¾› Analysis è¾“å‡ºæ–‡ä»¶ï¼Œä½¿ç”¨å†…ç½®ç¤ºä¾‹æ•°æ®")
    return DEFAULT_TEST_DATA


def main():
    """å®Œæ•´æµ‹è¯•æµç¨‹"""

    args = parse_args()
    cluster_summaries = load_cluster_summaries(args.analysis_json)

    print("=" * 80)
    print("å®Œæ•´æµ‹è¯•æµç¨‹ï¼šWriting Agent + Judge Agent")
    print("=" * 80)

    # ==================== Phase 1: ç”Ÿæˆè‰ç¨¿ ====================
    print("\n" + "=" * 80)
    print("Phase 1: ä½¿ç”¨ Writing Agent ç”Ÿæˆè‰ç¨¿")
    print("=" * 80)

    try:
        writer = WritingAgent(CONFIG, style="narrative")
        print("\nâœ“ Writing Agent åˆå§‹åŒ–æˆåŠŸ")

        print("\n[Step 1.1] ç”Ÿæˆ 3 ä¸ªå€™é€‰è‰ç¨¿...")
        candidates = writer.generate_multiple_candidates(
            cluster_summaries=cluster_summaries,
            num_candidates=3,
            temperature_range=(0.3, 0.7)
        )

        print(f"\nâœ… æˆåŠŸç”Ÿæˆ {len(candidates)} ä¸ªå€™é€‰è‰ç¨¿")

        # æ˜¾ç¤ºå€™é€‰è‰ç¨¿æ‘˜è¦
        print("\nå€™é€‰è‰ç¨¿æ‘˜è¦:")
        for i, candidate in enumerate(candidates, 1):
            content_preview = candidate['content'][:150].replace('\n', ' ')
            print(f"\n  è‰ç¨¿ {i}:")
            print(f"    æ¸©åº¦: {candidate['temperature']:.2f}")
            print(f"    å¼•ç”¨æ•°: {len(candidate['citations'])}")
            print(f"    é•¿åº¦: {len(candidate['content'])} å­—ç¬¦")
            print(f"    é¢„è§ˆ: {content_preview}...")

    except Exception as e:
        print(f"\nâŒ Phase 1 å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

    # ==================== Phase 2: è¯„ä¼°è‰ç¨¿ ====================
    print("\n\n" + "=" * 80)
    print("Phase 2: ä½¿ç”¨ Judge Agent è¯„ä¼°è‰ç¨¿")
    print("=" * 80)

    try:
        # åˆ›å»º Judge Agentï¼ˆä½¿ç”¨æ›´ä½çš„æ¸©åº¦ä»¥æé«˜è¯„åˆ†ä¸€è‡´æ€§ï¼‰
        judge_config = ModelConfig(
            name="GLM-4.6",  # ä¿®æ­£ï¼šå»æ‰ $ å‰ç¼€
            api_key="sk-aRG9iu2Hy9--oPxrG-5faA",
            base_url="https://llmapi.paratera.com/v1/",
            temperature=0.2,  # Judge ä½¿ç”¨ä½æ¸©åº¦
            max_tokens=4096
        )
        judge = JudgeAgent(judge_config)
        print("\nâœ“ Judge Agent åˆå§‹åŒ–æˆåŠŸ")

        print("\n[Step 2.1] å¯¹æ‰€æœ‰è‰ç¨¿è¿›è¡Œè¯„åˆ†å’Œæ’åº...")

        # æå–è‰ç¨¿å†…å®¹
        drafts = [c['content'] for c in candidates]

        # æ’åº
        ranked = judge.rank_drafts(drafts, reference=cluster_summaries)

        print(f"\nâœ… è¯„ä¼°å®Œæˆ")

        # æ˜¾ç¤ºæ’åºç»“æœ
        print("\næ’åºç»“æœ:")
        print("-" * 80)
        print(f"{'æ’å':<6} {'è‰ç¨¿ID':<10} {'æ€»åˆ†':<10} {'è¦†ç›–åº¦':<10} {'å‡†ç¡®æ€§':<10} {'è¿è´¯æ€§':<10}")
        print("-" * 80)

        for i, result in enumerate(ranked, 1):
            scores = result.get('scores', {})
            print(f"{i:<6} "
                  f"{result['draft_id']:<10} "
                  f"{result['overall_score']:<10.1f} "
                  f"{scores.get('coverage', 0):<10.1f} "
                  f"{scores.get('factuality', 0):<10.1f} "
                  f"{scores.get('coherence', 0):<10.1f}")

    except Exception as e:
        print(f"\nâŒ Phase 2 å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

    # ==================== Phase 3: é€‰æ‹©æœ€ä½³è‰ç¨¿ ====================
    print("\n\n" + "=" * 80)
    print("Phase 3: é€‰æ‹©æœ€ä½³è‰ç¨¿")
    print("=" * 80)

    try:
        best = ranked[0]

        print(f"\nğŸ† æœ€ä½³è‰ç¨¿ï¼šè‰ç¨¿ {best['draft_id']}")
        print(f"   æ€»åˆ†: {best['overall_score']:.1f}/100")
        print(f"   é•¿åº¦: {best['draft_length']} å­—ç¬¦")

        # æ˜¾ç¤ºå„ç»´åº¦å¾—åˆ†
        print(f"\nğŸ“Š å„ç»´åº¦å¾—åˆ†:")
        scores = best.get('scores', {})
        for dim, score in scores.items():
            bar = "â–ˆ" * int(score / 5) + "â–‘" * (20 - int(score / 5))
            print(f"   {dim:20s}: {score:3.0f}/100  {bar}")

        # æ˜¾ç¤ºä¼˜ç‚¹
        print(f"\nâœ… ä¼˜ç‚¹ ({len(best.get('strengths', []))}):")
        for i, strength in enumerate(best.get('strengths', []), 1):
            print(f"   {i}. {strength}")

        # æ˜¾ç¤ºç¼ºç‚¹
        print(f"\nâš ï¸  ç¼ºç‚¹ ({len(best.get('weaknesses', []))}):")
        for i, weakness in enumerate(best.get('weaknesses', []), 1):
            print(f"   {i}. {weakness}")

        # æ˜¾ç¤ºæ”¹è¿›å»ºè®®
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®® ({len(best.get('improvement_suggestions', []))}):")
        for i, suggestion in enumerate(best.get('improvement_suggestions', []), 1):
            print(f"   {i}. é—®é¢˜: {suggestion.get('issue', 'N/A')}")
            print(f"      å»ºè®®: {suggestion.get('suggestion', 'N/A')}")

    except Exception as e:
        print(f"\nâŒ Phase 3 å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

    # ==================== Phase 4: äº‹å®éªŒè¯ ====================
    print("\n\n" + "=" * 80)
    print("Phase 4: äº‹å®å‡†ç¡®æ€§éªŒè¯")
    print("=" * 80)

    try:
        # å‡†å¤‡è¯æ®ææ–™
        evidence = {}
        for cluster_data in cluster_summaries.values():
            for paper in cluster_data.get('papers', []):
                evidence[paper['paper_id']] = paper

        print(f"\n[Step 4.1] éªŒè¯æœ€ä½³è‰ç¨¿çš„äº‹å®å‡†ç¡®æ€§...")
        verification = judge.verify_factuality(best['draft'], evidence)

        print(f"\nâœ… éªŒè¯å®Œæˆ")

        # æ˜¾ç¤ºéªŒè¯ç»“æœ
        print(f"\nğŸ“Š éªŒè¯ç»“æœ:")
        print(f"   æ€»é™ˆè¿°æ•°: {verification.get('total_claims', 'N/A')}")
        print(f"   å·²éªŒè¯æ•°: {verification.get('verified_claims', 'N/A')}")
        print(f"   å‡†ç¡®ç‡: {verification.get('accuracy_rate', 0):.1%}")

        citation_check = verification.get('citation_check', {})
        print(f"\nğŸ“ å¼•ç”¨æ£€æŸ¥:")
        print(f"   æ€»å¼•ç”¨æ•°: {citation_check.get('total_citations', 0)}")
        print(f"   æ— æ•ˆå¼•ç”¨æ•°: {len(citation_check.get('invalid_citations', []))}")
        print(f"   å¼•ç”¨æœ‰æ•ˆç‡: {citation_check.get('citation_validity_rate', 0):.1%}")

        if citation_check.get('invalid_citations'):
            print(f"\nâš ï¸  æ— æ•ˆå¼•ç”¨:")
            for citation in citation_check['invalid_citations']:
                print(f"      - [{citation}]")
        else:
            print(f"\nâœ… æ‰€æœ‰å¼•ç”¨å‡æœ‰æ•ˆ")

    except Exception as e:
        print(f"\nâŒ Phase 4 å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

    # ==================== Phase 5: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š ====================
    print("\n\n" + "=" * 80)
    print("Phase 5: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
    print("=" * 80)

    try:
        final_report = {
            "summary": {
                "best_draft_id": best['draft_id'],
                "overall_score": best['overall_score'],
                "factuality_rate": verification.get('accuracy_rate', 0),
                "citation_validity": citation_check.get('citation_validity_rate', 0)
            },
            "best_draft": {
                "content": best['draft'],
                "scores": best.get('scores', {}),
                "length": best['draft_length']
            },
            "feedback": {
                "strengths": best.get('strengths', []),
                "weaknesses": best.get('weaknesses', []),
                "improvements": best.get('improvement_suggestions', [])
            },
            "verification": {
                "accuracy_rate": verification.get('accuracy_rate', 0),
                "citation_check": citation_check
            },
            "alternatives": [
                {
                    "draft_id": alt['draft_id'],
                    "score": alt['overall_score']
                }
                for alt in ranked[1:]
            ]
        }

        # ä¿å­˜æŠ¥å‘Š
        with open("full_pipeline_report.json", "w", encoding="utf-8") as f:
            json.dump(final_report, f, ensure_ascii=False, indent=2)

        # ä¿å­˜æœ€ä½³è‰ç¨¿
        with open("best_draft.txt", "w", encoding="utf-8") as f:
            f.write(best['draft'])

        print("\nâœ… æŠ¥å‘Šå·²ç”Ÿæˆ")
        print("\næ–‡ä»¶å·²ä¿å­˜:")
        print("   - full_pipeline_report.json  (å®Œæ•´è¯„ä¼°æŠ¥å‘Š)")
        print("   - best_draft.txt             (æœ€ä½³è‰ç¨¿)")

        # æ˜¾ç¤ºæ‘˜è¦
        print("\n" + "=" * 80)
        print("æµ‹è¯•æ‘˜è¦")
        print("=" * 80)
        print(f"\nğŸ¯ ç»¼åˆè¯„ä¼°:")
        print(f"   æ€»åˆ†: {final_report['summary']['overall_score']:.1f}/100")
        print(f"   äº‹å®å‡†ç¡®ç‡: {final_report['summary']['factuality_rate']:.1%}")
        print(f"   å¼•ç”¨æœ‰æ•ˆç‡: {final_report['summary']['citation_validity']:.1%}")

        print(f"\nğŸ“ è´¨é‡ç­‰çº§:")
        score = final_report['summary']['overall_score']
        if score >= 90:
            grade = "ä¼˜ç§€ï¼ˆå¯å‘è¡¨ï¼‰"
        elif score >= 80:
            grade = "è‰¯å¥½ï¼ˆéœ€å°å¹…ä¿®æ”¹ï¼‰"
        elif score >= 70:
            grade = "åˆæ ¼ï¼ˆéœ€ä¸­ç­‰ä¿®æ”¹ï¼‰"
        elif score >= 60:
            grade = "å°šå¯ï¼ˆéœ€å¤§å¹…ä¿®æ”¹ï¼‰"
        else:
            grade = "ä¸åˆæ ¼ï¼ˆéœ€é‡å†™ï¼‰"
        print(f"   ç­‰çº§: {grade}")

        print(f"\nâœ… å®Œæ•´æµ‹è¯•æµç¨‹æˆåŠŸå®Œæˆï¼")

        return final_report

    except Exception as e:
        print(f"\nâŒ Phase 5 å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()
