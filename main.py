import asyncio
import os
from dotenv import load_dotenv
from agents.retrieval_agent import RetrievalAgent

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

async def main():
    """
    å¯åŠ¨ Retrieval Agent è¿›è¡Œ LLM KV Cache æ–‡çŒ®æ£€ç´¢ã€‚
    """
    
    # ==========================================
    # 1. é…ç½®åŒºåŸŸ (Configuration)
    # ==========================================
    
    # ä¼˜å…ˆæ£€æŸ¥ DEEPSEEK_API_KEY
    api_key = os.getenv("DEEPSEEK_API_KEY")
    model_name = "deepseek-chat"
    base_url = "https://api.deepseek.com"

    # å¦‚æœæ²¡æœ‰ DeepSeek Keyï¼Œå°è¯•æ£€æŸ¥ OPENAI_API_KEY (ä½œä¸ºå¤‡é€‰)
    if not api_key:
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            print("âš ï¸ æœªæ‰¾åˆ° DEEPSEEK_API_KEY,åˆ‡æ¢ä¸º OpenAI é…ç½®...")
            model_name = "gpt-4o"
            base_url = None # ä½¿ç”¨å®˜æ–¹é»˜è®¤åœ°å€
        else:
            print("âŒ é”™è¯¯: æœªåœ¨ .env æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡ä¸­æ‰¾åˆ° API Key (DEEPSEEK_API_KEY æˆ– OPENAI_API_KEY)ã€‚")
            return

    # ==========================================
    # 2. åˆå§‹åŒ– Agent
    # ==========================================
    
    print(f"ğŸ¤– æ­£å†åˆå§‹åŒ– RetrievalAgent...")
    print(f"   - Model: {model_name}")
    print(f"   - Base URL: {base_url if base_url else 'Default'}")

    try:
        # æ˜¾å¼ä¼ å…¥æ‰€æœ‰é…ç½®ï¼Œä¸ä¾èµ–é»˜è®¤å€¼
        agent = RetrievalAgent(
            model=model_name,
            base_url=base_url,
            api_key=api_key
        )
    except Exception as e:
        print(f"âŒ Agent åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # ==========================================
    # 3. å®šä¹‰æ£€ç´¢ä»»åŠ¡ (Task Definition)
    # ==========================================
    
    # é’ˆå¯¹ä½ è¦æ±‚çš„ KV Cache ä¸»é¢˜ï¼Œæˆ‘è®¾è®¡äº†ä¸€ä¸ªè¯¦ç»†çš„ Research Query
    topic = (
        "Investigate the latest advancements (2023-2025) in LLM KV Cache optimization. "
        "Focus on key techniques such as: "
        "1. KV Cache Compression (Quantization, Sparse Attention). "
        "2. Eviction Policies (e.g., H2O, StreamingLLM, SnapKV). "
        "3. Efficient Memory Management for Long-Context Inference. "
        "Please provide a comprehensive summary of the state-of-the-art methods."
    )
    
    print(f"\nğŸ” å¼€å§‹æ‰§è¡Œæ£€ç´¢ä»»åŠ¡:\n{topic}\n")
    print("-" * 60)

    # ==========================================
    # 4. è¿è¡Œ Agent (Execution)
    # ==========================================
    
    try:
        # å¼‚æ­¥è¿è¡Œ Agent
        final_report = await agent.run(topic)
        
        # ==========================================
        # 5. è¾“å‡ºç»“æœ
        # ==========================================
        print("\n" + "="*60)
        print("ğŸ“ FINAL LITERATURE REVIEW REPORT")
        print("="*60)
        print(final_report)
        
        # å¯é€‰ï¼šä¿å­˜åˆ°æ–‡ä»¶
        with open("review_result.md", "w", encoding="utf-8") as f:
            f.write(final_report)
        print(f"\nâœ… ç»“æœå·²ä¿å­˜è‡³ review_result.md")
        
    except Exception as e:
        print(f"\nâŒ è¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())