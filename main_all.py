"""
å®Œæ•´æµ‹è¯•æµç¨‹ï¼šRetrieval -> Analysis -> Writing -> Judging -> Verification
ç‰ˆæœ¬ï¼šFinal Optimized (Detailed Logs + File Save)
"""

import asyncio
import argparse
import json
import os
import sys
import time
from datetime import datetime
from typing import Dict, Any, List
from dotenv import load_dotenv
import pandas as pd
import json as jsonlib

# ç¡®ä¿å½“å‰ç›®å½•ä¸çˆ¶ç›®å½•åœ¨æœç´¢è·¯å¾„ä¸­ï¼Œé¿å…ç›¸å¯¹å¯¼å…¥æŠ¥é”™
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PARENT_DIR = os.path.dirname(CURRENT_DIR)
for _p in (CURRENT_DIR, PARENT_DIR):
    if _p and _p not in sys.path:
        sys.path.insert(0, _p)

from agents.retrieval_agent import RetrievalAgent
from agents.analysis_agent import AnalysisAgent
from writing_judging.writing_agent import WritingAgent, ModelConfig
from writing_judging.judge_agent import JudgeAgent
from writing_judging.pipeline_adapter import analysis_to_cluster_summaries
from core.schema import LLMMessage


# ==================== é…ç½®ä¸ç¯å¢ƒ ====================
load_dotenv()
API_KEY = "sk-aRG9iu2Hy9--oPxrG-5faA"
BASE_URL = os.getenv("PARATERA_BASE_URL", "https://llmapi.paratera.com/v1/")
MODEL_NAME = os.getenv("PARATERA_MODEL", "DeepSeek-V3.2-Instruct")

if not API_KEY:
    print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° API Keyï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶ã€‚")

COMMON_AGENT_ARGS = {
    "model": MODEL_NAME,
    "base_url": BASE_URL,
    "api_key": API_KEY
}

WRITER_CONFIG = ModelConfig(
    name=MODEL_NAME, api_key=API_KEY, base_url=BASE_URL, temperature=0.7, max_tokens=8192
)

JUDGE_CONFIG = ModelConfig(
    name="GLM-4.6", api_key=API_KEY, base_url=BASE_URL, temperature=0.2, max_tokens=4096
)


def load_judge_models(config_path: str) -> List[ModelConfig]:
    """ä» model_config.json è½½å…¥æ¨¡å‹åˆ—è¡¨ï¼ˆä¸­æ–‡æ³¨é‡Šä¾¿äºé˜…è¯»ï¼‰ã€‚"""
    with open(config_path, "r", encoding="utf-8") as f:
        data = jsonlib.load(f)
    models = data.get("models", {})
    judge_models = []
    for _, info in models.items():
        judge_models.append(
            ModelConfig(
                name=info.get("name"),
                api_key=info.get("api_key"),
                base_url=info.get("base_url", BASE_URL),
                temperature=info.get("temperature", 0.2),
                max_tokens=info.get("max_tokens", 4096)
            )
        )
    return judge_models

# ==================== æ—¥å¿—ä¸ç›®å½•å·¥å…· ====================

class DualLogger(object):
    """åŒæ—¶å°†è¾“å‡ºå†™å…¥ç»ˆç«¯å’Œæ–‡ä»¶"""
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.log = open(filename, "a", encoding='utf-8')

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush() # ç¡®ä¿å®æ—¶å†™å…¥

    def flush(self):
        self.terminal.flush()
        self.log.flush()

def get_timestamp():
    return datetime.now().strftime("%Y%m%d_%H%M%S")

def create_run_directory(run_id: str):
    base_dir = os.path.join("output", run_id)
    traj_dir = os.path.join(base_dir, "trajectory")
    os.makedirs(traj_dir, exist_ok=True)
    return base_dir, traj_dir

def serialize_history(history: List[Any]) -> List[Dict]:
    serialized = []
    for msg in history:
        if hasattr(msg, 'role'):
            m_dict = {"role": msg.role, "content": msg.content}
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                m_dict["tool_calls"] = [{"name": tc.name, "arguments": tc.arguments} for tc in msg.tool_calls]
            if hasattr(msg, 'tool_call_id') and msg.tool_call_id:
                m_dict["tool_call_id"] = msg.tool_call_id
                m_dict["name"] = msg.name
            serialized.append(m_dict)
        else:
            serialized.append(msg if isinstance(msg, dict) else str(msg))
    return serialized

def save_json(data: Any, folder: str, filename: str):
    if not filename.endswith(".json"):
        filename += ".json"
    path = os.path.join(folder, filename)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return path

def parse_args():
    parser = argparse.ArgumentParser()
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--query", type=str, help="ä»å¤´å¼€å§‹ï¼šæŒ‡å®šç ”ç©¶ä¸»é¢˜")
    group.add_argument("--analysis-json", type=str, help="ä»ä¸­é—´å¼€å§‹ï¼šæŒ‡å®š analysis ç»“æœæ–‡ä»¶")
    group.add_argument("--dataset-pkl", type=str, help="ä½¿ç”¨æœ¬åœ° pkl æ•°æ®é›†ä½œä¸ºè®ºæ–‡æ¥æºï¼Œè·³è¿‡ç½‘ç»œæ£€ç´¢")
    parser.add_argument("--max-search-steps", type=int, default=3)
    parser.add_argument("--sample-n", type=int, default=-1, help="dataset-pkl æ¨¡å¼ä¸‹é‡‡æ ·æ¡æ•°ï¼Œ-1 è¡¨ç¤ºå…¨éƒ¨")
    parser.add_argument("--skip-n", type=int, default=0, help="dataset-pkl æ¨¡å¼ä¸‹è·³è¿‡å‰ N æ¡æ ·æœ¬")
    parser.add_argument("--use-test-only", action="store_true", help="dataset-pkl æ¨¡å¼ä¸‹ä»…ä½¿ç”¨ split == 'test' çš„æ ·æœ¬")
    parser.add_argument("--per-paper", action="store_true", help="dataset-pkl æ¨¡å¼ä¸‹é€ç¯‡å¤„ç†ï¼Œæ¯ç¯‡åˆ†é…ä¸€ä¸ª API Keyï¼ˆåˆ†æ/å†™ä½œ/å¤šæ¨¡å‹è¯„å®¡å…±ç”¨è¯¥ Keyï¼‰")
    parser.add_argument("--api-keys-file", type=str, help="å­˜æ”¾å¤šä¸ª API Key çš„æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªæˆ– JSON æ•°ç»„ï¼‰")
    return parser.parse_args()


def load_papers_from_pkl(pkl_path: str, sample_n: int = -1, use_test_only: bool = False) -> List[Dict[str, Any]]:
    """ä»æœ¬åœ° pkl åŠ è½½è®ºæ–‡åˆ—è¡¨ï¼Œæ¨¡æ‹Ÿæ£€ç´¢ç»“æœï¼ˆä¸­æ–‡æ³¨é‡Šæ–¹ä¾¿é˜…è¯»ï¼‰ã€‚"""
    df = pd.read_pickle(pkl_path)
    if use_test_only and "split" in df.columns:
        df = df[df["split"] == "test"]
    if sample_n and sample_n > 0:
        df = df.head(sample_n)
    papers = []
    for idx, row in df.iterrows():
        papers.append({
            "id": str(idx),
            "title": row.get("title", ""),
            "abstract": row.get("abstract", ""),
            "authors": [],
            "year": None,
            "summary": row.get("abstract", "")
        })
    return papers


def log_progress(step: int, total: int, label: str):
    """ç®€å•è¿›åº¦æ¡æ‰“å°ï¼ˆä¸­æ–‡æ³¨é‡Šä¾¿äºé˜…è¯»ï¼‰ã€‚"""
    bar_len = 30
    filled = int(bar_len * step / total)
    bar = "â–ˆ" * filled + "â–‘" * (bar_len - filled)
    print(f"[è¿›åº¦ {step}/{total}] {bar} {label}")


def render_progress(current: int, total: int, start_ts: float, label: str):
    """å•è¡Œè¿›åº¦æ¡ï¼Œé™„å‰©ä½™æ—¶é—´é¢„ä¼°ï¼ˆä¸­æ–‡æ³¨é‡Šä¾¿äºé˜…è¯»ï¼‰ã€‚"""
    elapsed = time.time() - start_ts
    rate = elapsed / current if current else 0
    remaining = rate * (total - current) if rate else 0
    bar_len = 30
    filled = int(bar_len * current / total)
    bar = "â–ˆ" * filled + "â–‘" * (bar_len - filled)
    eta = datetime.fromtimestamp(time.time() + remaining).strftime("%H:%M:%S") if current else "--:--:--"
    sys.stdout.write(f"\r[{label}] {current}/{total} {bar} ETA {eta}")
    sys.stdout.flush()
    if current == total:
        sys.stdout.write("\n")


def load_api_keys(path: str) -> List[str]:
    """ä»æ–‡ä»¶è¯»å– API Key åˆ—è¡¨ï¼Œæ”¯æŒæ¯è¡Œæˆ– JSON æ•°ç»„ï¼ˆä¸­æ–‡æ³¨é‡Šä¾¿äºé˜…è¯»ï¼‰ã€‚"""
    if not path or not os.path.exists(path):
        return []
    text = open(path, "r", encoding="utf-8").read().strip()
    keys: List[str] = []
    try:
        data = jsonlib.loads(text)
        if isinstance(data, list):
            keys = [str(k).strip() for k in data]
    except Exception:
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        parsed = []
        for line in lines:
            if "=" in line:
                line = line.split("=", 1)[1].strip()
            parsed.append(line)
        keys = parsed
    # ä»…ä¿ç•™ sk- å¼€å¤´çš„ key
    keys = [k for k in keys if k.startswith("sk-")]
    return keys


def build_configs(api_key: str) -> Dict[str, Any]:
    """æŒ‰ç»™å®š key æ„é€ æ£€ç´¢/å†™ä½œ/è¯„å®¡é…ç½®ï¼ˆä¸­æ–‡æ³¨é‡Šä¾¿äºé˜…è¯»ï¼‰ã€‚"""
    common = {
        "model": MODEL_NAME,
        "base_url": BASE_URL,
        "api_key": api_key
    }
    writer_conf = ModelConfig(
        name=MODEL_NAME, api_key=api_key, base_url=BASE_URL, temperature=0.7, max_tokens=8192
    )
    judge_conf = ModelConfig(
        name="GLM-4.6", api_key=api_key, base_url=BASE_URL, temperature=0.2, max_tokens=4096
    )
    return {"common": common, "writer": writer_conf, "judge": judge_conf}

# ==================== ä¸»æµç¨‹ ====================

async def main():
    args = parse_args()
    
    # 1. ç›®å½•åˆå§‹åŒ–
    RUN_ID = get_timestamp()
    RUN_DIR, TRAJ_DIR = create_run_directory(RUN_ID)

    # 2. ã€å…³é”®ã€‘é‡å®šå‘ print åˆ°æ—¥å¿—æ–‡ä»¶
    log_file_path = os.path.join(RUN_DIR, "execution_log.txt")
    sys.stdout = DualLogger(log_file_path)
    
    print("=" * 80)
    print(f"ğŸš€ å…¨è‡ªåŠ¨æ–‡çŒ®ç»¼è¿°æµæ°´çº¿å¯åŠ¨")
    print(f"ğŸ“ æœ¬æ¬¡è¿è¡Œç›®å½•: {RUN_DIR}")
    print(f"ğŸ“ å®Œæ•´æ—¥å¿—æ–‡ä»¶: {log_file_path}")
    print("=" * 80)

    analysis_result = {}
    cluster_summaries = {}
    papers = []

    # ------------------------------------------------------------------
    # Phase 1 & 2: æ•°æ®è·å– (Retrieval + Analysis)
    # ------------------------------------------------------------------
    if args.per_paper and not args.dataset_pkl:
        print("âŒ per-paper æ¨¡å¼éœ€é…åˆ --dataset-pkl ä½¿ç”¨ã€‚")
        return

    if args.per_paper:
        api_keys = load_api_keys(args.api_keys_file)
        if not api_keys:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ API Key åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ --api-keys-fileã€‚")
            return
        papers = load_papers_from_pkl(args.dataset_pkl, sample_n=args.sample_n, use_test_only=args.use_test_only)
        if args.skip_n and args.skip_n > 0:
            papers = papers[args.skip_n:]
        if not papers:
            print("âŒ æ•°æ®é›†ä¸ºç©ºæˆ–è§£æå¤±è´¥ã€‚")
            return

        total_papers = len(papers)
        start_index = args.skip_n or 0
        run_start = time.time()

        # æ¯ä¸ª API ä¸€ä¸ª workerï¼Œä¸²è¡Œå¤„ç†åˆ†é…ç»™å®ƒçš„å¤šç¯‡æ–‡ç« ï¼›å¤šä¸ª API å¹¶è¡Œ
        task_queue = asyncio.Queue()
        for idx, paper in enumerate(papers):
            # é€»è¾‘ç¼–å·ä» 1 å¼€å§‹ï¼Œå åŠ  skip åç§»ï¼Œä¾¿äºä¸åŸå§‹åºå·å¯¹é½
            logical_idx = idx + start_index
            task_queue.put_nowait((logical_idx, paper))
        completed = 0
        completed_lock = asyncio.Lock()

        async def process_article(idx: int, paper: Dict[str, Any], key: str):
            cfgs = build_configs(key)
            display_current = idx + 1  # idx å·²åŒ…å« skip åç§»
            display_total = total_papers + start_index
            print(f"\n=== å¤„ç†ç¬¬ {display_current}/{display_total} ç¯‡ï¼Œä½¿ç”¨æ¨¡å‹ Key: {key[:6]}... ===")

            # åˆ†æ
            analyzer = AnalysisAgent(datas=[paper], **cfgs["common"])
            analysis_result = await analyzer.run(f"Dataset-Article-{paper.get('id')}")
            # å…¼å®¹è¿”å› list çš„æƒ…å†µï¼Œå–é¦–ä¸ªå…ƒç´ 
            if isinstance(analysis_result, list):
                analysis_result = analysis_result[0] if analysis_result else {}
            cluster_summaries = analysis_to_cluster_summaries(analysis_result, [paper])

            # å†™ä½œ 3 ç¯‡ï¼ˆæ¸©åº¦ 0.3/0.4/0.5 å„ 1 ç¯‡ï¼‰
            writer = WritingAgent(cfgs["writer"], style="narrative")
            candidates, cid = [], 1
            temps = [0.3, 0.4, 0.5]
            writing_start = time.time()
            for t in temps:
                draft = await writer.generate_draft_async(cluster_summaries=cluster_summaries, temperature=t)
                draft["candidate_id"] = cid
                cid += 1
                candidates.append(draft)
                render_progress(len(candidates), 3, writing_start, "å†™ä½œè¿›åº¦")

            # è¯„å®¡ï¼ˆåŒä¸€ Key è°ƒç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ‰€æœ‰æ¨¡å‹ï¼Œå–å¹³å‡ï¼‰
            judge_model_configs = load_judge_models(os.path.join(CURRENT_DIR, "model_config.json"))
            if not judge_model_configs:
                print("âŒ æ— å¯ç”¨è¯„å®¡æ¨¡å‹ï¼ˆmodel_config.json ä¸ºç©ºï¼‰ã€‚")
                return
            judge_model_configs = [
                ModelConfig(
                    name=mc.name,
                    api_key=key,  # ç”¨å½“å‰æ–‡ç« çš„ key è¦†ç›–
                    base_url=mc.base_url,
                    temperature=mc.temperature,
                    max_tokens=mc.max_tokens,
                    top_p=mc.top_p,
                )
                for mc in judge_model_configs
            ]

            draft_texts = [c['content'] for c in candidates]
            eval_start = time.time()
            evals = []
            for j, draft in enumerate(draft_texts, 1):
                # è®¡ç®—è‡ªåŠ¨æŒ‡æ ‡ï¼ˆå¼•æ–‡ç²¾åº¦/å¬å›/F1/å‡†ç¡®ç‡ï¼‰
                pred_refs = set(candidates[j-1].get("citations", [])) if j-1 < len(candidates) else set()
                human_refs = {
                    paper.get("paper_id")
                    for cluster in cluster_summaries.values()
                    for paper in cluster.get("papers", [])
                    if paper.get("paper_id")
                }
                matches = len(pred_refs & human_refs)
                prec = matches / len(pred_refs) if pred_refs else 0.0
                rec = matches / len(human_refs) if human_refs else 0.0
                f1 = 2 * prec * rec / (prec + rec) if prec + rec > 0 else 0.0
                # å†…å®¹ã€ç»“æ„æŒ‡æ ‡å ä½ï¼šå¯æ¥å…¥è¯­ä¹‰ç›¸ä¼¼åº¦/ç»“æ„åŒ¹é…æ¨¡å‹
                content_metrics = {
                    "semantic_similarity": 0.0,  # TODO: æ¥å…¥å‘é‡ç›¸ä¼¼åº¦
                    "rouge_l": 0.0,              # TODO: æ¥å…¥ Rouge-L
                    "kpr": 0.0                    # TODO: æ¥å…¥å…³é”®ç‚¹å¬å›
                }
                structure_metrics = {
                    "overlap": 0.0,        # TODO: ç»“æ„é‡å åº¦
                    "relevance_llm": 0.0   # TODO: LLM ç»“æ„ç›¸å…³æ€§ 1-5
                }
                auto_metrics = {
                    "citation": {
                        "precision": prec,
                        "recall": rec,
                        "f1": f1,
                        "accuracy": prec,
                    },
                    "content": content_metrics,
                    "structure": structure_metrics,
                }

                # å¼‚æ­¥å¹¶è¡Œå¤šæ¨¡å‹è¯„åˆ†ï¼Œå–å‡åˆ†
                async def eval_model(cfg: ModelConfig):
                    agent = JudgeAgent(cfg)
                    res = await agent.evaluate_draft_async(draft, {"task": "Dataset-Article", "clusters": cluster_summaries})
                    # å°†è‡ªåŠ¨æŒ‡æ ‡èåˆåˆ°æœ€ç»ˆåˆ†æ•°ï¼ˆ50% æ¨¡å‹å‡åˆ† + 50% å¼•æ–‡æŒ‡æ ‡ï¼‰
                    # è¿™é‡Œå…ˆè¿”å›æ¨¡å‹åˆ†ï¼Œåç»­è®¡ç®—å‡åˆ†å†ä¸ auto_metrics èåˆ
                    return cfg.name, res.get("overall_score")

                results = await asyncio.gather(*[eval_model(cfg) for cfg in judge_model_configs])
                model_scores = {name: score for name, score in results if score is not None}
                avg_score = round(sum(model_scores.values()) / len(model_scores), 2) if model_scores else 0.0

                # ä»…ä½¿ç”¨å¼•æ–‡è‡ªåŠ¨æŒ‡æ ‡ï¼Œæƒé‡ 30%ï¼›æ¨¡å‹å‡åˆ†æƒé‡ 70%
                citation_score = auto_metrics["citation"]["f1"] * 100
                auto_overall = citation_score  # å†…å®¹/ç»“æ„å ä½ä¸º 0

                final_score = round(avg_score * 0.7 + auto_overall * 0.3, 2)

                evals.append({
                    "draft_id": j-1,
                    "draft": draft,
                    "model_scores": model_scores,
                    "auto_metrics": auto_metrics,
                    "vote_final_score": final_score
                })
                render_progress(j, len(draft_texts), eval_start, "è¯„å®¡è¿›åº¦")

            ranked_results = sorted(evals, key=lambda x: x.get("vote_final_score") or 0, reverse=True)

            # è¾“å‡ºå½“å‰ç¯‡ç»“æœ
            save_json(cluster_summaries, RUN_DIR, f"article_{idx+1}_cluster_summaries")
            save_json(candidates, RUN_DIR, f"article_{idx+1}_writing_candidates")
            save_json(ranked_results, RUN_DIR, f"article_{idx+1}_judging_result")
            best = ranked_results[0]
            # ä¿å­˜æœ€ä½³è‰ç¨¿å®Œæ•´å†…å®¹
            save_json(best, RUN_DIR, f"article_{idx+1}_best")
            with open(os.path.join(RUN_DIR, f"article_{idx+1}_best.txt"), "w", encoding="utf-8") as f:
                f.write(best.get("draft", ""))
            print(f"\nç¬¬ {idx+1} ç¯‡æœ€ä½³è‰ç¨¿å¾—åˆ†: {best['vote_final_score']} (æ¨¡å‹å‡åˆ† {best['model_scores']})")

        async def worker(key: str):
            nonlocal completed
            while not task_queue.empty():
                try:
                    idx, paper = task_queue.get_nowait()
                except asyncio.QueueEmpty:
                    return
                await process_article(idx, paper, key)
                task_queue.task_done()
                async with completed_lock:
                    completed += 1
                    render_progress(completed, total_papers, run_start, "æ–‡ç« è¿›åº¦")

        workers = []
        for key in api_keys:
            workers.append(worker(key))

        await asyncio.gather(*workers)
        print("\nâœ… æ‰€æœ‰æ–‡ç« å¤„ç†å®Œæˆã€‚")
        return

    if args.query:
        # >>> Phase 1: Retrieval <<<
        print("\n" + "=" * 80)
        print("Phase 1: Retrieval Agent (æ–‡çŒ®æ£€ç´¢)")
        print("=" * 80)
        
        retriever = RetrievalAgent(**COMMON_AGENT_ARGS)
        papers = await retriever.run(args.query, max_steps=args.max_search_steps)
        
        save_json(serialize_history(retriever.history), TRAJ_DIR, "retrieval_traj")
        
        if not papers:
            print("âŒ æ£€ç´¢å¤±è´¥ï¼Œæœªæ‰¾åˆ°è®ºæ–‡ã€‚")
            return
        save_json(papers, RUN_DIR, "1_retrieval_papers")
        print(f"âœ… æ£€ç´¢å®Œæˆï¼Œè·å– {len(papers)} ç¯‡è®ºæ–‡ã€‚")
        log_progress(1, 5, "å®Œæˆæ£€ç´¢")

    elif args.dataset_pkl:
        print(f"\nğŸ“‚ Loading papers from dataset: {args.dataset_pkl}")
        papers = load_papers_from_pkl(args.dataset_pkl, sample_n=args.sample_n, use_test_only=args.use_test_only)
        if not papers:
            print("âŒ æ•°æ®é›†ä¸ºç©ºæˆ–è§£æå¤±è´¥ã€‚")
            return
        save_json(papers, RUN_DIR, "1_retrieval_papers_dataset")
        print(f"âœ… æˆåŠŸè½½å…¥ {len(papers)} ç¯‡è®ºæ–‡ï¼ˆæœ¬åœ°æ•°æ®é›†ï¼‰ã€‚")
        log_progress(1, 5, "å®Œæˆæ•°æ®è½½å…¥")

    else:
        print(f"\nğŸ“‚ Loading analysis from {args.analysis_json}")
        with open(args.analysis_json, "r", encoding="utf-8") as f:
            analysis_result = json.load(f)
        papers = analysis_result.get("datas", [])
        cluster_summaries = analysis_to_cluster_summaries(analysis_result)

    # >>> Phase 2: Analysis <<<
    if not cluster_summaries:
        print("\n" + "=" * 80)
        print("Phase 2: Analysis Agent (æ·±åº¦åˆ†æ)")
        print("=" * 80)

        analyzer = AnalysisAgent(datas=papers, **COMMON_AGENT_ARGS)
        analysis_task = args.query or (f"Dataset-{os.path.basename(args.dataset_pkl)}" if args.dataset_pkl else "Analysis")
        analysis_result = await analyzer.run(analysis_task)

        save_json(serialize_history(analyzer.history), TRAJ_DIR, "analysis_traj")

        if not analysis_result:
            print("âŒ åˆ†æå¤±è´¥ã€‚")
            return
        save_json(analysis_result, RUN_DIR, "2_analysis_result")
        print(f"âœ… åˆ†æå®Œæˆï¼Œç”Ÿæˆ {len(analysis_result.get('clusters', []))} ä¸ªç ”ç©¶èšç±»ã€‚")

        print("\nğŸ”„ Adapting data format...")
        cluster_summaries = analysis_to_cluster_summaries(analysis_result, papers)
        log_progress(2, 5, "å®Œæˆåˆ†æ")

    if not cluster_summaries:
        print("âŒ æ— æ³•è·å– Cluster Summariesï¼Œæµç¨‹ç»ˆæ­¢ã€‚")
        return
    
    save_json(cluster_summaries, RUN_DIR, "3_adapter_input")
    log_progress(3, 5, "å®Œæˆæ ¼å¼é€‚é…")

    # ------------------------------------------------------------------
    # Phase 3: å†™ä½œ (Writing)
    # ------------------------------------------------------------------
    print("\n" + "=" * 80)
    print("Phase 3: ä½¿ç”¨ Writing Agent ç”Ÿæˆè‰ç¨¿")
    print("=" * 80)

    try:
        writer = WritingAgent(WRITER_CONFIG, style="narrative")
        print("\nâœ“ Writing Agent åˆå§‹åŒ–æˆåŠŸ")
        
        print("\n[Step 3.1] æŒ‰æ¸©åº¦ç”Ÿæˆ 9 ä¸ªå€™é€‰è‰ç¨¿ (é¡ºåºå¼‚æ­¥ï¼Œå¸¦è¿›åº¦)...")
        temps = [0.3, 0.4, 0.5]
        candidates = []
        cid = 1
        total_writing = len(temps) * 3
        writing_start = time.time()
        for t in temps:
            for _ in range(3):
                draft = await writer.generate_draft_async(
                    cluster_summaries=cluster_summaries,
                    temperature=t
                )
                draft["candidate_id"] = cid
                candidates.append(draft)
                cid += 1
                render_progress(len(candidates), total_writing, writing_start, "å†™ä½œè¿›åº¦")
        
        if not candidates:
            print("âŒ å†™ä½œå¤±è´¥ã€‚")
            return
        save_json(candidates, RUN_DIR, "4_writing_candidates")
        
        print(f"\nâœ… æˆåŠŸç”Ÿæˆ {len(candidates)} ä¸ªå€™é€‰è‰ç¨¿")
        log_progress(4, 5, "å®Œæˆå†™ä½œ")

        # === è¯¦ç»†æ‰“å° ===
        print("\nå€™é€‰è‰ç¨¿æ‘˜è¦:")
        for i, candidate in enumerate(candidates, 1):
            content_preview = candidate['content'][:150].replace('\n', ' ')
            print(f"\n  è‰ç¨¿ {i}:")
            print(f"    æ¸©åº¦: {candidate.get('temperature', 'N/A'):.2f}")
            print(f"    å¼•ç”¨æ•°: {len(candidate.get('citations', []))}")
            print(f"    é•¿åº¦: {len(candidate.get('content', ''))} å­—ç¬¦")
            print(f"    é¢„è§ˆ: {content_preview}...")
        # ===============

    except Exception as e:
        print(f"\nâŒ Phase 3 Error: {e}")
        import traceback; traceback.print_exc()
        return

    # ------------------------------------------------------------------
    # Phase 4: è¯„å®¡ä¸æ‹©ä¼˜ (Judging)
    # ------------------------------------------------------------------
    print("\n\n" + "=" * 80)
    print("Phase 4: ä½¿ç”¨ Judge Agent è¯„ä¼°è‰ç¨¿")
    print("=" * 80)

    try:
        reference_material = {
            "task": args.query or "Analysis",
            "clusters": cluster_summaries
        }
        draft_texts = [c['content'] for c in candidates]
        
        print("\n[Step 4.1] å¯¹æ‰€æœ‰è‰ç¨¿è¿›è¡Œè¯„åˆ†å’Œæ’åº (æ–‡ç« çº§å¹¶è¡Œï¼Œæ¯ç¯‡åˆ†é…ä¸€ä¸ªæ¨¡å‹)...")
        judge_model_configs = load_judge_models(os.path.join(CURRENT_DIR, "model_config.json"))
        if not judge_model_configs:
            print("âŒ æ— å¯ç”¨è¯„å®¡æ¨¡å‹ï¼ˆæœªé…ç½®æœ‰æ•ˆ sk- å¼€å¤´çš„ api_keyï¼‰ã€‚")
            return

        # é¢„åˆ›å»º Agentï¼Œè½®è¯¢åˆ†é…è‰ç¨¿
        judge_agents = [JudgeAgent(cfg) for cfg in judge_model_configs]
        print(f"âœ“ åŠ è½½ {len(judge_agents)} ä¸ªè¯„å®¡æ¨¡å‹")

        evals = []
        total_eval = len(draft_texts)
        eval_start = time.time()

        async def eval_one(idx: int, draft: str, agent: JudgeAgent):
            res = await agent.evaluate_draft_async(draft, reference_material)
            return {
                "draft_id": idx,
                "draft": draft,
                "model_scores": {agent.model_config.name: res.get("overall_score")},
                "vote_final_score": res.get("overall_score")
            }

        tasks = []
        for idx, draft in enumerate(draft_texts):
            agent = judge_agents[idx % len(judge_agents)]
            tasks.append(eval_one(idx, draft, agent))

        for i, coro in enumerate(asyncio.as_completed(tasks), 1):
            evaluation = await coro
            evals.append(evaluation)
            render_progress(i, total_eval, eval_start, "è¯„å®¡è¿›åº¦")

        ranked_results = sorted(evals, key=lambda x: x.get("vote_final_score") or 0, reverse=True)

        # å†™å…¥æ¨¡å‹æ‰“åˆ†æ˜ç»†
        model_scores = [
            {
                "draft_id": res["draft_id"],
                "model_scores": res.get("model_scores", {}),
                "final_score": res.get("vote_final_score")
            }
            for res in ranked_results
        ]
        save_json(model_scores, RUN_DIR, "5_model_scores")
        print("\næ¨¡å‹æ‰“åˆ†æ˜ç»†:")
        for item in model_scores:
            print(f"è‰ç¨¿ {item['draft_id']} æ¨¡å‹è¯„åˆ†: {item['model_scores']} æœ€ç»ˆ: {item['final_score']}")
        
        # æ¢å¤å…ƒæ•°æ®
        for rank_item in ranked_results:
            idx = rank_item['draft_id']
            if idx < len(candidates):
                rank_item['meta'] = {
                    "temp": candidates[idx].get("temperature"),
                    "citations": len(candidates[idx].get("citations", []))
                }

        best = ranked_results[0]
        
        save_json(ranked_results, RUN_DIR, "5_judging_result")
        
        print(f"\nâœ… è¯„ä¼°å®Œæˆ")
        log_progress(5, 5, "å®Œæˆè¯„å®¡")

        # === è¯¦ç»†æ‰“å° ===
        print("\næ’åºç»“æœ:")
        print("-" * 80)
        print(f"{'æ’å':<6} {'è‰ç¨¿ID':<10} {'æ€»åˆ†':<10} {'è¦†ç›–åº¦':<10} {'å‡†ç¡®æ€§':<10} {'è¿è´¯æ€§':<10}")
        print("-" * 80)

        for i, result in enumerate(ranked_results, 1):
            scores = result.get('scores', {})
            print(f"{i:<6} "
                  f"{result['draft_id']:<10} "
                  f"{result['overall_score']:<10.1f} "
                  f"{scores.get('coverage', 0):<10.1f} "
                  f"{scores.get('factuality', 0):<10.1f} "
                  f"{scores.get('coherence', 0):<10.1f}")
        
        # æœ€ä½³è‰ç¨¿è¯¦æƒ…
        print("\n\n" + "=" * 80)
        print("Phase 4.5: æœ€ä½³è‰ç¨¿è¯¦æƒ…")
        print("=" * 80)
        
        print(f"\nğŸ† æœ€ä½³è‰ç¨¿ï¼šè‰ç¨¿ {best['draft_id']}")
        print(f"   æ€»åˆ†: {best['overall_score']:.1f}/100")
        print(f"   é•¿åº¦: {best.get('draft_length', 0)} å­—ç¬¦")

        print(f"\nğŸ“Š å„ç»´åº¦å¾—åˆ†:")
        scores = best.get('scores', {})
        for dim, score in scores.items():
            bar = "â–ˆ" * int(score / 5) + "â–‘" * (20 - int(score / 5))
            print(f"   {dim:20s}: {score:3.0f}/100  {bar}")

        print(f"\nâœ… ä¼˜ç‚¹ ({len(best.get('strengths', []))}):")
        for i, strength in enumerate(best.get('strengths', []), 1):
            print(f"   {i}. {strength}")

        print(f"\nâš ï¸  ç¼ºç‚¹ ({len(best.get('weaknesses', []))}):")
        for i, weakness in enumerate(best.get('weaknesses', []), 1):
            print(f"   {i}. {weakness}")

        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®® ({len(best.get('improvement_suggestions', []))}):")
        for i, suggestion in enumerate(best.get('improvement_suggestions', []), 1):
            print(f"   {i}. é—®é¢˜: {suggestion.get('issue', 'N/A')}")
            print(f"      å»ºè®®: {suggestion.get('suggestion', 'N/A')}")
        # ===============

    except Exception as e:
        print(f"\nâŒ Phase 4 Error: {e}")
        import traceback; traceback.print_exc()
        return

    # ------------------------------------------------------------------
    # Phase 5: éªŒè¯ä¸äº¤ä»˜ (Verification)
    # ------------------------------------------------------------------
    print("\n\n" + "=" * 80)
    print("Phase 5: äº‹å®å‡†ç¡®æ€§éªŒè¯ä¸æœ€ç»ˆæŠ¥å‘Š")
    print("=" * 80)

    try:
        evidence = {}
        for p in papers:
            if p.get("id"): evidence[p["id"]] = p
            if p.get("url"): evidence[p["url"]] = p
        if not evidence:
            for c in cluster_summaries.values():
                for p in c.get("papers", []):
                    evidence[p["paper_id"]] = p

        print(f"\n[Step 5.1] éªŒè¯æœ€ä½³è‰ç¨¿çš„äº‹å®å‡†ç¡®æ€§...")
        verification = judge.verify_factuality(best['draft'], evidence)
        
        print(f"\nâœ… éªŒè¯å®Œæˆ")

        # === è¯¦ç»†æ‰“å° ===
        print(f"\nğŸ“Š éªŒè¯ç»“æœ:")
        print(f"   æ€»é™ˆè¿°æ•°: {verification.get('total_claims', 'N/A')}")
        print(f"   å·²éªŒè¯æ•°: {verification.get('verified_claims', 'N/A')}")
        print(f"   å‡†ç¡®ç‡: {verification.get('accuracy_rate', 0):.1%}")

        citation_check = verification.get('citation_check', {})
        print(f"\nğŸ“ å¼•ç”¨æ£€æŸ¥:")
        print(f"   æ€»å¼•ç”¨æ•°: {citation_check.get('total_citations', 0)}")
        print(f"   æ— æ•ˆå¼•ç”¨æ•°: {len(citation_check.get('invalid_citations', []))}")
        print(f"   å¼•ç”¨æœ‰æ•ˆç‡: {citation_check.get('citation_validity_rate', 0):.1%}")

        if citation_check.get('invalid_citations'):
            print(f"\nâš ï¸  æ— æ•ˆå¼•ç”¨:")
            for citation in citation_check['invalid_citations']:
                print(f"      - [{citation}]")
        else:
            print(f"\nâœ… æ‰€æœ‰å¼•ç”¨å‡æœ‰æ•ˆ")
        # ===============

        # æ„å»ºæœ€ç»ˆæŠ¥å‘Š
        final_report = {
            "meta": {"run_id": RUN_ID, "timestamp": datetime.now().isoformat()},
            "best_draft_content": best['draft'],
            "evaluation": {
                "score": best['overall_score'],
                "details": best.get("scores"),
                "feedback": {
                    "strengths": best.get("strengths"),
                    "weaknesses": best.get("weaknesses"),
                    "improvements": best.get("improvement_suggestions")
                }
            },
            "verification": verification
        }
        
        save_json(final_report, RUN_DIR, "FINAL_REPORT")
        
        md_path = os.path.join(RUN_DIR, "FINAL_PAPER.md")
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(f"# Literature Review: {args.query or 'Auto Survey'}\n\n")
            f.write(f"> Run ID: {RUN_ID} | Score: {best['overall_score']:.1f}\n\n")
            f.write(best['draft'])
            f.write("\n\n---\n*Verified by Judge Agent*")
        
        print("\n\n" + "=" * 80)
        print("æµ‹è¯•æ‘˜è¦")
        print("=" * 80)
        print(f"\nğŸ¯ ç»¼åˆè¯„ä¼°:")
        print(f"   æ€»åˆ†: {best['overall_score']:.1f}/100")
        print(f"   äº‹å®å‡†ç¡®ç‡: {verification.get('accuracy_rate', 0):.1%}")
        print(f"   å¼•ç”¨æœ‰æ•ˆç‡: {citation_check.get('citation_validity_rate', 0):.1%}")

        print(f"\nğŸ“ è´¨é‡ç­‰çº§:")
        score = best['overall_score']
        if score >= 90: grade = "ä¼˜ç§€ï¼ˆå¯å‘è¡¨ï¼‰"
        elif score >= 80: grade = "è‰¯å¥½ï¼ˆéœ€å°å¹…ä¿®æ”¹ï¼‰"
        elif score >= 70: grade = "åˆæ ¼ï¼ˆéœ€ä¸­ç­‰ä¿®æ”¹ï¼‰"
        elif score >= 60: grade = "å°šå¯ï¼ˆéœ€å¤§å¹…ä¿®æ”¹ï¼‰"
        else: grade = "ä¸åˆæ ¼ï¼ˆéœ€é‡å†™ï¼‰"
        print(f"   ç­‰çº§: {grade}")

        print(f"\nâœ… å®Œæ•´æµ‹è¯•æµç¨‹æˆåŠŸå®Œæˆï¼")
        print(f"   ç»“æœç›®å½•: {RUN_DIR}")

    except Exception as e:
        print(f"\nâŒ Phase 5 Error: {e}")
        import traceback; traceback.print_exc()

if __name__ == "__main__":
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())

    # python main.py --query "Coding Benchmark for LLM in Repo-Level" --max-search-steps 5
    # python main.py --analysis-json "ä½ çš„åˆ†æç»“æœjsonè·¯å¾„"
