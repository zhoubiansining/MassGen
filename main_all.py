"""
å®Œæ•´æµ‹è¯•æµç¨‹ï¼šRetrieval -> Analysis -> Writing -> Judging -> Verification
ç‰ˆæœ¬ï¼šFinal Optimized (Detailed Logs + File Save)
"""

import asyncio
import argparse
import json
import re
import os
import sys
import time
from datetime import datetime
from typing import Dict, Any, List
from dotenv import load_dotenv
import pandas as pd
import json as jsonlib

# ç¡®ä¿å½“å‰ç›®å½•ä¸çˆ¶ç›®å½•åœ¨æœç´¢è·¯å¾„ä¸­ï¼Œé¿å…ç›¸å¯¹å¯¼å…¥æŠ¥é”™
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PARENT_DIR = os.path.dirname(CURRENT_DIR)
for _p in (CURRENT_DIR, PARENT_DIR):
    if _p and _p not in sys.path:
        sys.path.insert(0, _p)

from agents.retrieval_agent import RetrievalAgent
from agents.analysis_agent import AnalysisAgent
from writing_judging.writing_agent import WritingAgent, ModelConfig
from writing_judging.judge_agent import JudgeAgent
from writing_judging.pipeline_adapter import analysis_to_cluster_summaries
from core.schema import LLMMessage


# ==================== é…ç½®ä¸ç¯å¢ƒ ====================
load_dotenv()
API_KEY = "sk-aRG9iu2Hy9--oPxrG-5faA"
BASE_URL = os.getenv("PARATERA_BASE_URL", "https://llmapi.paratera.com/v1/")
MODEL_NAME = os.getenv("PARATERA_MODEL", "Qwen3-235B-A22B-Instruct-2507")

if not API_KEY:
    print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° API Keyï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶ã€‚")

COMMON_AGENT_ARGS = {
    "model": MODEL_NAME,
    "base_url": BASE_URL,
    "api_key": API_KEY
}

WRITER_CONFIG = ModelConfig(
    name=MODEL_NAME, api_key=API_KEY, base_url=BASE_URL, temperature=0.7, max_tokens=8192
)

JUDGE_CONFIG = ModelConfig(
    name="GLM-4.6", api_key=API_KEY, base_url=BASE_URL, temperature=0.2, max_tokens=4096
)


def load_judge_models(config_path: str) -> List[ModelConfig]:
    """ä» model_config.json è½½å…¥æ¨¡å‹åˆ—è¡¨ï¼ˆä¸­æ–‡æ³¨é‡Šä¾¿äºé˜…è¯»ï¼‰ã€‚"""
    with open(config_path, "r", encoding="utf-8") as f:
        data = jsonlib.load(f)
    models = data.get("models", {})
    judge_models = []
    for _, info in models.items():
        judge_models.append(
            ModelConfig(
                name=info.get("name"),
                api_key=info.get("api_key"),
                base_url=info.get("base_url", BASE_URL),
                temperature=info.get("temperature", 0.2),
                max_tokens=info.get("max_tokens", 4096)
            )
        )
    return judge_models

# ==================== æ—¥å¿—ä¸ç›®å½•å·¥å…· ====================

class DualLogger(object):
    """åŒæ—¶å°†è¾“å‡ºå†™å…¥ç»ˆç«¯å’Œæ–‡ä»¶"""
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.log = open(filename, "a", encoding='utf-8')

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush() # ç¡®ä¿å®æ—¶å†™å…¥

    def flush(self):
        self.terminal.flush()
        self.log.flush()

def get_timestamp():
    return datetime.now().strftime("%Y%m%d_%H%M%S")

def create_run_directory(run_id: str):
    base_dir = os.path.join("output", run_id)
    traj_dir = os.path.join(base_dir, "trajectory")
    os.makedirs(traj_dir, exist_ok=True)
    return base_dir, traj_dir

def serialize_history(history: List[Any]) -> List[Dict]:
    serialized = []
    for msg in history:
        if hasattr(msg, 'role'):
            m_dict = {"role": msg.role, "content": msg.content}
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                m_dict["tool_calls"] = [{"name": tc.name, "arguments": tc.arguments} for tc in msg.tool_calls]
            if hasattr(msg, 'tool_call_id') and msg.tool_call_id:
                m_dict["tool_call_id"] = msg.tool_call_id
                m_dict["name"] = msg.name
            serialized.append(m_dict)
        else:
            serialized.append(msg if isinstance(msg, dict) else str(msg))
    return serialized

def save_json(data: Any, folder: str, filename: str):
    if not filename.endswith(".json"):
        filename += ".json"
    path = os.path.join(folder, filename)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return path

def parse_args():
    parser = argparse.ArgumentParser()
    group = parser.add_mutually_exclusive_group(required=False)
    group.add_argument("--query", type=str, help="ä»å¤´å¼€å§‹ï¼šæŒ‡å®šç ”ç©¶ä¸»é¢˜")
    group.add_argument("--analysis-json", type=str, help="ä»ä¸­é—´å¼€å§‹ï¼šæŒ‡å®š analysis ç»“æœæ–‡ä»¶")
    group.add_argument("--dataset-pkl", type=str, help="ä½¿ç”¨æœ¬åœ° pkl æ•°æ®é›†ä½œä¸ºâ€œæ£€ç´¢ç§å­â€ï¼ˆä»…æå–æ ‡é¢˜å‘èµ·æ£€ç´¢ï¼‰")
    parser.add_argument("--max-search-steps", type=int, default=3)
    parser.add_argument("--sample-n", type=int, default=-1, help="dataset-pkl æ¨¡å¼ä¸‹é‡‡æ ·æ¡æ•°ï¼Œ-1 è¡¨ç¤ºå…¨éƒ¨")
    parser.add_argument("--skip-n", type=int, default=0, help="dataset-pkl æ¨¡å¼ä¸‹è·³è¿‡å‰ N æ¡æ ·æœ¬")
    parser.add_argument("--use-test-only", action="store_true", help="dataset-pkl æ¨¡å¼ä¸‹ä»…ä½¿ç”¨ split == 'test' çš„æ ·æœ¬")
    parser.add_argument("--per-paper", action="store_true", help="dataset-pkl æ¨¡å¼ä¸‹é€ç¯‡å¤„ç†ï¼Œæ¯ç¯‡åˆ†é…ä¸€ä¸ª API Keyï¼ˆåˆ†æ/å†™ä½œ/å¤šæ¨¡å‹è¯„å®¡å…±ç”¨è¯¥ Keyï¼‰")
    parser.add_argument("--api-keys-file", type=str, help="å­˜æ”¾å¤šä¸ª API Key çš„æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªæˆ– JSON æ•°ç»„ï¼‰")
    parser.add_argument("--retrieval-only", action="store_true", help="ä»…æ‰§è¡Œæ£€ç´¢å¹¶ä¿å­˜ç»“æœï¼Œä¸è¿›å…¥åˆ†æ/å†™ä½œ/è¯„å®¡")
    parser.add_argument("--reuse-retrieval-dir", type=str, help="ä½¿ç”¨æŒ‡å®šç›®å½•ä¸‹çš„ title_<idx>_retrieval.json ä½œä¸ºæ£€ç´¢ç»“æœï¼Œè·³è¿‡åœ¨çº¿æ£€ç´¢")
    return parser.parse_args()


def load_papers_from_pkl(pkl_path: str, sample_n: int = -1, use_test_only: bool = False) -> List[Dict[str, Any]]:
    """ä»æœ¬åœ° pkl åŠ è½½è®ºæ–‡åˆ—è¡¨ï¼Œæ¨¡æ‹Ÿæ£€ç´¢ç»“æœï¼ˆä¸­æ–‡æ³¨é‡Šæ–¹ä¾¿é˜…è¯»ï¼‰ã€‚"""
    df = pd.read_pickle(pkl_path)
    if use_test_only and "split" in df.columns:
        df = df[df["split"] == "test"]
    if sample_n and sample_n > 0:
        df = df.head(sample_n)
    papers = []
    for idx, row in df.iterrows():
        papers.append({
            "id": str(idx),
            "title": row.get("title", ""),
            "abstract": row.get("abstract", ""),
            "authors": [],
            "year": None,
            "summary": row.get("abstract", "")
        })
    return papers


def load_titles_from_pkl(pkl_path: str, sample_n: int = -1, use_test_only: bool = False) -> List[str]:
    """ä»æœ¬åœ° pkl åŠ è½½æ ‡é¢˜åˆ—è¡¨ï¼ˆç”¨äºå‘èµ·æ£€ç´¢ï¼‰ã€‚"""
    df = pd.read_pickle(pkl_path)
    if use_test_only and "split" in df.columns:
        df = df[df["split"] == "test"]
    if sample_n and sample_n > 0:
        df = df.head(sample_n)
    return [str(t) for t in df.get("title", []) if str(t).strip()]


def log_progress(step: int, total: int, label: str):
    """ç®€å•è¿›åº¦æ¡æ‰“å°ï¼ˆä¸­æ–‡æ³¨é‡Šä¾¿äºé˜…è¯»ï¼‰ã€‚"""
    bar_len = 30
    filled = int(bar_len * step / total)
    bar = "â–ˆ" * filled + "â–‘" * (bar_len - filled)
    print(f"[è¿›åº¦ {step}/{total}] {bar} {label}")


def render_progress(current: int, total: int, start_ts: float, label: str):
    """å•è¡Œè¿›åº¦æ¡ï¼Œé™„å‰©ä½™æ—¶é—´é¢„ä¼°ï¼ˆä¸­æ–‡æ³¨é‡Šä¾¿äºé˜…è¯»ï¼‰ã€‚"""
    elapsed = time.time() - start_ts
    rate = elapsed / current if current else 0
    remaining = rate * (total - current) if rate else 0
    bar_len = 30
    filled = int(bar_len * current / total)
    bar = "â–ˆ" * filled + "â–‘" * (bar_len - filled)
    eta = datetime.fromtimestamp(time.time() + remaining).strftime("%H:%M:%S") if current else "--:--:--"
    sys.stdout.write(f"\r[{label}] {current}/{total} {bar} ETA {eta}")
    sys.stdout.flush()
    if current == total:
        sys.stdout.write("\n")


def load_api_keys(path: str) -> List[str]:
    """ä»æ–‡ä»¶è¯»å– API Key åˆ—è¡¨ï¼Œæ”¯æŒæ¯è¡Œæˆ– JSON æ•°ç»„ï¼ˆä¸­æ–‡æ³¨é‡Šä¾¿äºé˜…è¯»ï¼‰ã€‚"""
    if not path or not os.path.exists(path):
        return []
    text = open(path, "r", encoding="utf-8").read().strip()
    keys: List[str] = []
    try:
        data = jsonlib.loads(text)
        if isinstance(data, list):
            keys = [str(k).strip() for k in data]
    except Exception:
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        parsed = []
        for line in lines:
            if "=" in line:
                line = line.split("=", 1)[1].strip()
            parsed.append(line)
        keys = parsed
    # ä»…ä¿ç•™ sk- å¼€å¤´çš„ key
    keys = [k for k in keys if k.startswith("sk-")]
    return keys


def build_configs(api_key: str) -> Dict[str, Any]:
    """æŒ‰ç»™å®š key æ„é€ æ£€ç´¢/å†™ä½œ/è¯„å®¡é…ç½®ï¼ˆä¸­æ–‡æ³¨é‡Šä¾¿äºé˜…è¯»ï¼‰ã€‚"""
    common = {
        "model": MODEL_NAME,
        "base_url": BASE_URL,
        "api_key": api_key
    }
    writer_conf = ModelConfig(
        name=MODEL_NAME, api_key=api_key, base_url=BASE_URL, temperature=0.7, max_tokens=8192
    )
    judge_conf = ModelConfig(
        name="GLM-4.6", api_key=api_key, base_url=BASE_URL, temperature=0.2, max_tokens=4096
    )
    return {"common": common, "writer": writer_conf, "judge": judge_conf}

# ==================== ä¸»æµç¨‹ ====================

async def main():
    args = parse_args()
    if not (args.query or args.analysis_json or args.dataset_pkl or args.reuse_retrieval_dir):
        print("âŒ éœ€è¦æä¾› --query / --analysis-json / --dataset-pkl / --reuse-retrieval-dir ä¹‹ä¸€ã€‚")
        return
    
    # 1. ç›®å½•åˆå§‹åŒ–
    RUN_ID = get_timestamp()
    RUN_DIR, TRAJ_DIR = create_run_directory(RUN_ID)

    # 2. ã€å…³é”®ã€‘é‡å®šå‘ print åˆ°æ—¥å¿—æ–‡ä»¶
    log_file_path = os.path.join(RUN_DIR, "execution_log.txt")
    sys.stdout = DualLogger(log_file_path)
    
    print("=" * 80)
    print(f"ğŸš€ å…¨è‡ªåŠ¨æ–‡çŒ®ç»¼è¿°æµæ°´çº¿å¯åŠ¨")
    print(f"ğŸ“ æœ¬æ¬¡è¿è¡Œç›®å½•: {RUN_DIR}")
    print(f"ğŸ“ å®Œæ•´æ—¥å¿—æ–‡ä»¶: {log_file_path}")
    print("=" * 80)

    analysis_result = {}
    cluster_summaries = {}
    papers = []

    # ------------------------------------------------------------------
    # Phase 1 & 2: æ•°æ®è·å– (Retrieval + Analysis)
    # ------------------------------------------------------------------
    if args.per_paper and not (args.dataset_pkl or args.reuse_retrieval_dir):
        print("âŒ per-paper æ¨¡å¼éœ€æä¾› --dataset-pkl æˆ– --reuse-retrieval-dirã€‚")
        return

    if args.per_paper:
        api_keys = load_api_keys(args.api_keys_file)
        if not api_keys:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ API Key åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ --api-keys-fileã€‚")
            return

        # ä½¿ç”¨æµ‹è¯•é›†æ ‡é¢˜ä½œä¸ºæ£€ç´¢èµ·ç‚¹ï¼ˆå¼ºåˆ¶åªå– testï¼‰
        # å¤„ç† reuse_retrieval_dirï¼ˆç›´æ¥å¤ç”¨å·²ä¸‹è½½çš„æ£€ç´¢ç»“æœï¼‰
        reuse_entries = []
        reuse_ids: List[int] = []
        if args.reuse_retrieval_dir:
            import glob
            pattern = os.path.join(args.reuse_retrieval_dir, "title_*_retrieval.json")
            files = sorted(
                glob.glob(pattern), 
                key=lambda x: int(re.search(r"title_(\d+)_retrieval", os.path.basename(x)).group(1)) 
                              if re.search(r"title_(\d+)_retrieval", os.path.basename(x)) else 0
            )
            for path in files:
                try:
                    data = json.load(open(path, "r", encoding="utf-8"))
                    fname = os.path.basename(path)
                    m = re.search(r"title_(\d+)_retrieval", fname)
                    file_id = int(m.group(1)) if m else len(reuse_entries) + 1
                    if isinstance(data, dict):
                        title_val = data.get("title") or os.path.basename(path)
                        papers_val = data.get("papers") or []
                    else:
                        title_val = os.path.basename(path)
                        papers_val = data
                    reuse_entries.append((title_val, papers_val))
                    reuse_ids.append(file_id)
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•è¯»å– {path}: {e}")

        if args.reuse_retrieval_dir and not reuse_entries:
            print("âŒ reuse_retrieval_dir æœªæ‰¾åˆ°ä»»ä½• title_*_retrieval.json")
            return

        if args.reuse_retrieval_dir:
            titles = [t for t, _ in reuse_entries]
            preloaded_papers = [p for _, p in reuse_entries]
        else:
            titles = load_titles_from_pkl(
                args.dataset_pkl,
                sample_n=args.sample_n,
                use_test_only=True,
            )
            preloaded_papers = None

        if args.skip_n and args.skip_n > 0:
            titles = titles[args.skip_n:]
            if preloaded_papers:
                preloaded_papers = preloaded_papers[args.skip_n:]
            if args.reuse_retrieval_dir and reuse_ids:
                reuse_ids = reuse_ids[args.skip_n:]
        if not titles:
            print("âŒ æ•°æ®é›†ä¸ºç©ºæˆ–æœªæ‰¾åˆ°æ ‡é¢˜ã€‚")
            return

        total_titles = len(titles)
        start_index = args.skip_n or 0
        run_start = time.time()
        print(f"ğŸ“‘ ä»æµ‹è¯•é›†æå– {total_titles} ä¸ªæ ‡é¢˜ï¼Œå‡†å¤‡å¯åŠ¨æ£€ç´¢")

        task_queue = asyncio.Queue()
        for idx, title in enumerate(titles):
            logical_idx = idx + start_index
            item_id = reuse_ids[idx] if args.reuse_retrieval_dir and reuse_ids and idx < len(reuse_ids) else logical_idx + 1
            papers_prefill = preloaded_papers[idx] if preloaded_papers and idx < len(preloaded_papers) else None
            task_queue.put_nowait((logical_idx, item_id, title, papers_prefill))
        completed = 0
        completed_lock = asyncio.Lock()

        async def process_title(display_idx: int, item_id: int, title: str, papers_prefill: Any, key: str):
            cfgs = build_configs(key)
            display_current = display_idx + 1
            display_total = total_titles + start_index
            print(f"\n=== å¤„ç†ç¬¬ {display_current}/{display_total} ä¸ªæ ‡é¢˜ï¼Œä½¿ç”¨æ¨¡å‹ Key: {key[:6]}... ===")

            # Phase 1: æ£€ç´¢æˆ–å¤ç”¨
            if papers_prefill is not None:
                papers_local = papers_prefill
            else:
                retriever = RetrievalAgent(**cfgs["common"])
                papers_local = await retriever.run(title, max_steps=args.max_search_steps)
                if not papers_local:
                    print("âŒ æ£€ç´¢æ— ç»“æœï¼Œè·³è¿‡è¯¥æ ‡é¢˜ã€‚")
                    return

            # è¡¥é½ ID
            for i, p in enumerate(papers_local):
                if not p.get("id"):
                    p["id"] = f"p{i+1}"
                if not p.get("paper_id"):
                    p["paper_id"] = p["id"]

            # ä¿å­˜æ£€ç´¢ç»“æœï¼Œå¸¦ä¸Šæ ‡é¢˜å…ƒæ•°æ®
            save_json({"title": title, "papers": papers_local}, RUN_DIR, f"title_{item_id}_retrieval")

            # ä»…æ£€ç´¢æ¨¡å¼ï¼Œæå‰è¿”å›
            if args.retrieval_only:
                return

            # Phase 2: åˆ†æ
            analyzer = AnalysisAgent(datas=papers_local, **cfgs["common"])
            analysis_result = await analyzer.run(title)
            if isinstance(analysis_result, list):
                analysis_result = analysis_result[0] if analysis_result else {}
            if not analysis_result:
                print("âŒ åˆ†æå¤±è´¥ï¼Œè·³è¿‡è¯¥æ ‡é¢˜ã€‚")
                return
            cluster_summaries_local = analysis_to_cluster_summaries(analysis_result, papers_local)

            # å¦‚æœç”Ÿæˆçš„ summaries ä»ç„¶ç¼ºå¤± paper_id æˆ–åŒ…å« tool call æ®‹ç•™ï¼Œåšä¸€æ¬¡ç®€å•å…œåº•
            def _invalid_cluster(cs: Dict[str, Any]) -> bool:
                if not cs:
                    return True
                topics = list(cs.values())
                if not topics:
                    return True
                topic_txt = topics[0].get("summary") or ""
                return "<function_calls>" in topic_txt or any(p.get("paper_id") is None for c in topics for p in c.get("papers", []))

            if _invalid_cluster(cluster_summaries_local):
                print("âš ï¸ åˆ†æç»“æœå¼‚å¸¸ï¼Œä½¿ç”¨å…œåº• cluster_summariesã€‚")
                cluster_summaries_local = {
                    "cluster_0": {
                        "topic": f"Survey seed: {title}",
                        "summary": f"Papers retrieved for: {title}",
                        "papers": [
                            {
                                "paper_id": p.get("paper_id"),
                                "title": p.get("title"),
                                "authors": p.get("authors", []),
                                "year": p.get("year") or p.get("published"),
                                "key_contribution": p.get("summary") or p.get("abstract", ""),
                                "abstract": p.get("abstract") or p.get("summary"),
                                "url": p.get("url") or p.get("link"),
                            }
                            for p in papers_local
                        ],
                    }
                }
            save_json(analysis_result, RUN_DIR, f"title_{item_id}_analysis")
            save_json(cluster_summaries_local, RUN_DIR, f"title_{item_id}_adapter_input")

            # Phase 3: å†™ä½œï¼ˆæ¸©åº¦ 0.3/0.4/0.5 å„ 1 ç¯‡ï¼‰
            writer = WritingAgent(cfgs["writer"], style="narrative")
            candidates, cid = [], 1
            temps = [0.3, 0.4, 0.5]
            writing_start = time.time()
            for t in temps:
                draft = await writer.generate_draft_async(
                    cluster_summaries=cluster_summaries_local,
                    temperature=t,
                )
                draft["candidate_id"] = cid
                cid += 1
                candidates.append(draft)
                render_progress(len(candidates), len(temps), writing_start, "å†™ä½œè¿›åº¦")

            if not candidates:
                print("âŒ å†™ä½œå¤±è´¥ï¼Œè·³è¿‡è¯¥æ ‡é¢˜ã€‚")
                return
            save_json(candidates, RUN_DIR, f"title_{item_id}_writing_candidates")

            # Phase 4: è¯„å®¡ï¼ˆå¤šæ¨¡å‹è¯„åˆ† + å¼•æ–‡æŒ‡æ ‡èåˆï¼‰
            judge_model_configs = load_judge_models(os.path.join(CURRENT_DIR, "model_config.json"))
            if not judge_model_configs:
                print("âŒ æ— å¯ç”¨è¯„å®¡æ¨¡å‹ï¼ˆmodel_config.json ä¸ºç©ºï¼‰ã€‚")
                return
            judge_model_configs = [
                ModelConfig(
                    name=mc.name,
                    api_key=key,
                    base_url=mc.base_url,
                    temperature=mc.temperature,
                    max_tokens=mc.max_tokens,
                    top_p=mc.top_p,
                )
                for mc in judge_model_configs
            ]

            draft_texts = [c['content'] for c in candidates]
            eval_start = time.time()
            evals = []
            human_refs = {
                paper.get("paper_id")
                for cluster in cluster_summaries_local.values()
                for paper in cluster.get("papers", [])
                if paper.get("paper_id")
            }

            async def eval_one(idx_c: int, draft: str, cand_meta: Dict):
                # è‡ªåŠ¨æŒ‡æ ‡ï¼ˆä»…å¼•æ–‡ï¼‰
                pred_refs = set(cand_meta.get("citations", []))
                matches = len(pred_refs & human_refs)
                prec = matches / len(pred_refs) if pred_refs else 0.0
                rec = matches / len(human_refs) if human_refs else 0.0
                f1 = 2 * prec * rec / (prec + rec) if prec + rec > 0 else 0.0

                auto_metrics = {
                    "citation": {
                        "precision": prec,
                        "recall": rec,
                        "f1": f1,
                        "accuracy": prec,
                    },
                    "content": {},
                    "structure": {},
                }

                # å¤šæ¨¡å‹è¯„åˆ†å¹¶å–å‡å€¼
                async def eval_model(cfg: ModelConfig):
                    agent = JudgeAgent(cfg)
                    res = await agent.evaluate_draft_async(draft, {"task": title, "clusters": cluster_summaries_local})
                    return cfg.name, res.get("overall_score"), res.get("scores")

                results = await asyncio.gather(*[eval_model(cfg) for cfg in judge_model_configs])
                model_scores = {
                    name: {
                        "overall": overall,
                        "dimensions": dims or {}
                    }
                    for name, overall, dims in results
                    if overall is not None
                }
                avg_score = round(sum(ms.get("overall", 0) for ms in model_scores.values()) / len(model_scores), 2) if model_scores else 0.0

                # èåˆï¼š70% æ¨¡å‹å‡åˆ† + 30% å¼•æ–‡ F1ï¼ˆ0-100ï¼‰
                citation_score = auto_metrics["citation"]["f1"] * 100
                final_score = round(avg_score * 0.7 + citation_score * 0.3, 2)

                return {
                    "draft_id": idx_c,
                    "draft": draft,
                    "model_scores": model_scores,
                    "auto_metrics": auto_metrics,
                    "vote_final_score": final_score,
                }

            tasks = [eval_one(j, draft, candidates[j]) for j, draft in enumerate(draft_texts)]

            for i, coro in enumerate(asyncio.as_completed(tasks), 1):
                evaluation = await coro
                evals.append(evaluation)
                render_progress(i, len(draft_texts), eval_start, "è¯„å®¡è¿›åº¦")

            ranked_results = sorted(evals, key=lambda x: x.get("vote_final_score") or 0, reverse=True)
            save_json(ranked_results, RUN_DIR, f"title_{item_id}_judging_result")

            best = ranked_results[0]
            save_json(best, RUN_DIR, f"title_{item_id}_best")
            with open(os.path.join(RUN_DIR, f"title_{item_id}_best.txt"), "w", encoding="utf-8") as f:
                f.write(best.get("draft", ""))

            print(f"\nç¬¬ {display_current} ä¸ªæ ‡é¢˜æœ€ä½³è‰ç¨¿å¾—åˆ†: {best['vote_final_score']} (æ¨¡å‹å‡åˆ† {best.get('model_scores')})")

        async def worker(key: str):
            nonlocal completed
            while not task_queue.empty():
                try:
                    display_idx, item_id, title, papers_prefill = task_queue.get_nowait()
                except asyncio.QueueEmpty:
                    return
                await process_title(display_idx, item_id, title, papers_prefill, key)
                task_queue.task_done()
                async with completed_lock:
                    completed += 1
                    render_progress(completed, total_titles, run_start, "æ ‡é¢˜è¿›åº¦")

        await asyncio.gather(*(worker(k) for k in api_keys))
        print("\nâœ… æ‰€æœ‰æ ‡é¢˜å¤„ç†å®Œæˆã€‚")
        return

    if args.query:
        # >>> Phase 1: Retrieval <<<
        print("\n" + "=" * 80)
        print("Phase 1: Retrieval Agent (æ–‡çŒ®æ£€ç´¢)")
        print("=" * 80)
        
        retriever = RetrievalAgent(**COMMON_AGENT_ARGS)
        papers = await retriever.run(args.query, max_steps=args.max_search_steps)
        
        save_json(serialize_history(retriever.history), TRAJ_DIR, "retrieval_traj")
        
        if not papers:
            print("âŒ æ£€ç´¢å¤±è´¥ï¼Œæœªæ‰¾åˆ°è®ºæ–‡ã€‚")
            return
        save_json(papers, RUN_DIR, "1_retrieval_papers")
        print(f"âœ… æ£€ç´¢å®Œæˆï¼Œè·å– {len(papers)} ç¯‡è®ºæ–‡ã€‚")
        log_progress(1, 5, "å®Œæˆæ£€ç´¢")

    elif args.dataset_pkl:
        print(f"\nğŸ“‚ Loading papers from dataset: {args.dataset_pkl}")
        papers = load_papers_from_pkl(args.dataset_pkl, sample_n=args.sample_n, use_test_only=args.use_test_only)
        if not papers:
            print("âŒ æ•°æ®é›†ä¸ºç©ºæˆ–è§£æå¤±è´¥ã€‚")
            return
        save_json(papers, RUN_DIR, "1_retrieval_papers_dataset")
        print(f"âœ… æˆåŠŸè½½å…¥ {len(papers)} ç¯‡è®ºæ–‡ï¼ˆæœ¬åœ°æ•°æ®é›†ï¼‰ã€‚")
        log_progress(1, 5, "å®Œæˆæ•°æ®è½½å…¥")

    else:
        print(f"\nğŸ“‚ Loading analysis from {args.analysis_json}")
        with open(args.analysis_json, "r", encoding="utf-8") as f:
            analysis_result = json.load(f)
        papers = analysis_result.get("datas", [])
        cluster_summaries = analysis_to_cluster_summaries(analysis_result)

    # >>> Phase 2: Analysis <<<
    if not cluster_summaries:
        print("\n" + "=" * 80)
        print("Phase 2: Analysis Agent (æ·±åº¦åˆ†æ)")
        print("=" * 80)

        analyzer = AnalysisAgent(datas=papers, **COMMON_AGENT_ARGS)
        analysis_task = args.query or (f"Dataset-{os.path.basename(args.dataset_pkl)}" if args.dataset_pkl else "Analysis")
        analysis_result = await analyzer.run(analysis_task)

        save_json(serialize_history(analyzer.history), TRAJ_DIR, "analysis_traj")

        if not analysis_result:
            print("âŒ åˆ†æå¤±è´¥ã€‚")
            return
        save_json(analysis_result, RUN_DIR, "2_analysis_result")
        print(f"âœ… åˆ†æå®Œæˆï¼Œç”Ÿæˆ {len(analysis_result.get('clusters', []))} ä¸ªç ”ç©¶èšç±»ã€‚")

        print("\nğŸ”„ Adapting data format...")
        cluster_summaries = analysis_to_cluster_summaries(analysis_result, papers)
        log_progress(2, 5, "å®Œæˆåˆ†æ")

    if not cluster_summaries:
        print("âŒ æ— æ³•è·å– Cluster Summariesï¼Œæµç¨‹ç»ˆæ­¢ã€‚")
        return
    
    save_json(cluster_summaries, RUN_DIR, "3_adapter_input")
    log_progress(3, 5, "å®Œæˆæ ¼å¼é€‚é…")

    # ------------------------------------------------------------------
    # Phase 3: å†™ä½œ (Writing)
    # ------------------------------------------------------------------
    print("\n" + "=" * 80)
    print("Phase 3: ä½¿ç”¨ Writing Agent ç”Ÿæˆè‰ç¨¿")
    print("=" * 80)

    try:
        writer = WritingAgent(WRITER_CONFIG, style="narrative")
        print("\nâœ“ Writing Agent åˆå§‹åŒ–æˆåŠŸ")
        
        print("\n[Step 3.1] æŒ‰æ¸©åº¦ç”Ÿæˆ 9 ä¸ªå€™é€‰è‰ç¨¿ (é¡ºåºå¼‚æ­¥ï¼Œå¸¦è¿›åº¦)...")
        temps = [0.3, 0.4, 0.5]
        candidates = []
        cid = 1
        total_writing = len(temps) * 3
        writing_start = time.time()
        for t in temps:
            for _ in range(3):
                draft = await writer.generate_draft_async(
                    cluster_summaries=cluster_summaries,
                    temperature=t
                )
                draft["candidate_id"] = cid
                candidates.append(draft)
                cid += 1
                render_progress(len(candidates), total_writing, writing_start, "å†™ä½œè¿›åº¦")
        
        if not candidates:
            print("âŒ å†™ä½œå¤±è´¥ã€‚")
            return
        save_json(candidates, RUN_DIR, "4_writing_candidates")
        
        print(f"\nâœ… æˆåŠŸç”Ÿæˆ {len(candidates)} ä¸ªå€™é€‰è‰ç¨¿")
        log_progress(4, 5, "å®Œæˆå†™ä½œ")

        # === è¯¦ç»†æ‰“å° ===
        print("\nå€™é€‰è‰ç¨¿æ‘˜è¦:")
        for i, candidate in enumerate(candidates, 1):
            content_preview = candidate['content'][:150].replace('\n', ' ')
            print(f"\n  è‰ç¨¿ {i}:")
            print(f"    æ¸©åº¦: {candidate.get('temperature', 'N/A'):.2f}")
            print(f"    å¼•ç”¨æ•°: {len(candidate.get('citations', []))}")
            print(f"    é•¿åº¦: {len(candidate.get('content', ''))} å­—ç¬¦")
            print(f"    é¢„è§ˆ: {content_preview}...")
        # ===============

    except Exception as e:
        print(f"\nâŒ Phase 3 Error: {e}")
        import traceback; traceback.print_exc()
        return

    # ------------------------------------------------------------------
    # Phase 4: è¯„å®¡ä¸æ‹©ä¼˜ (Judging)
    # ------------------------------------------------------------------
    print("\n\n" + "=" * 80)
    print("Phase 4: ä½¿ç”¨ Judge Agent è¯„ä¼°è‰ç¨¿")
    print("=" * 80)

    try:
        reference_material = {
            "task": args.query or "Analysis",
            "clusters": cluster_summaries
        }
        draft_texts = [c['content'] for c in candidates]
        
        print("\n[Step 4.1] å¯¹æ‰€æœ‰è‰ç¨¿è¿›è¡Œè¯„åˆ†å’Œæ’åº (æ–‡ç« çº§å¹¶è¡Œï¼Œæ¯ç¯‡åˆ†é…ä¸€ä¸ªæ¨¡å‹)...")
        judge_model_configs = load_judge_models(os.path.join(CURRENT_DIR, "model_config.json"))
        if not judge_model_configs:
            print("âŒ æ— å¯ç”¨è¯„å®¡æ¨¡å‹ï¼ˆæœªé…ç½®æœ‰æ•ˆ sk- å¼€å¤´çš„ api_keyï¼‰ã€‚")
            return

        # é¢„åˆ›å»º Agentï¼Œè½®è¯¢åˆ†é…è‰ç¨¿
        judge_agents = [JudgeAgent(cfg) for cfg in judge_model_configs]
        print(f"âœ“ åŠ è½½ {len(judge_agents)} ä¸ªè¯„å®¡æ¨¡å‹")

        evals = []
        total_eval = len(draft_texts)
        eval_start = time.time()

        async def eval_one(idx: int, draft: str, agent: JudgeAgent):
            res = await agent.evaluate_draft_async(draft, reference_material)
            return {
                "draft_id": idx,
                "draft": draft,
                "model_scores": {agent.model_config.name: res.get("overall_score")},
                "vote_final_score": res.get("overall_score")
            }

        tasks = []
        for idx, draft in enumerate(draft_texts):
            agent = judge_agents[idx % len(judge_agents)]
            tasks.append(eval_one(idx, draft, agent))

        for i, coro in enumerate(asyncio.as_completed(tasks), 1):
            evaluation = await coro
            evals.append(evaluation)
            render_progress(i, total_eval, eval_start, "è¯„å®¡è¿›åº¦")

        ranked_results = sorted(evals, key=lambda x: x.get("vote_final_score") or 0, reverse=True)

        # å†™å…¥æ¨¡å‹æ‰“åˆ†æ˜ç»†
        model_scores = [
            {
                "draft_id": res["draft_id"],
                "model_scores": res.get("model_scores", {}),
                "final_score": res.get("vote_final_score")
            }
            for res in ranked_results
        ]
        save_json(model_scores, RUN_DIR, "5_model_scores")
        print("\næ¨¡å‹æ‰“åˆ†æ˜ç»†:")
        for item in model_scores:
            print(f"è‰ç¨¿ {item['draft_id']} æ¨¡å‹è¯„åˆ†: {item['model_scores']} æœ€ç»ˆ: {item['final_score']}")
        
        # æ¢å¤å…ƒæ•°æ®
        for rank_item in ranked_results:
            idx = rank_item['draft_id']
            if idx < len(candidates):
                rank_item['meta'] = {
                    "temp": candidates[idx].get("temperature"),
                    "citations": len(candidates[idx].get("citations", []))
                }

        best = ranked_results[0]
        
        save_json(ranked_results, RUN_DIR, "5_judging_result")
        
        print(f"\nâœ… è¯„ä¼°å®Œæˆ")
        log_progress(5, 5, "å®Œæˆè¯„å®¡")

        # === è¯¦ç»†æ‰“å° ===
        print("\næ’åºç»“æœ:")
        print("-" * 80)
        print(f"{'æ’å':<6} {'è‰ç¨¿ID':<10} {'æ€»åˆ†':<10} {'è¦†ç›–åº¦':<10} {'å‡†ç¡®æ€§':<10} {'è¿è´¯æ€§':<10}")
        print("-" * 80)

        for i, result in enumerate(ranked_results, 1):
            scores = result.get('scores', {})
            print(f"{i:<6} "
                  f"{result['draft_id']:<10} "
                  f"{result['overall_score']:<10.1f} "
                  f"{scores.get('coverage', 0):<10.1f} "
                  f"{scores.get('factuality', 0):<10.1f} "
                  f"{scores.get('coherence', 0):<10.1f}")
        
        # æœ€ä½³è‰ç¨¿è¯¦æƒ…
        print("\n\n" + "=" * 80)
        print("Phase 4.5: æœ€ä½³è‰ç¨¿è¯¦æƒ…")
        print("=" * 80)
        
        print(f"\nğŸ† æœ€ä½³è‰ç¨¿ï¼šè‰ç¨¿ {best['draft_id']}")
        print(f"   æ€»åˆ†: {best['overall_score']:.1f}/100")
        print(f"   é•¿åº¦: {best.get('draft_length', 0)} å­—ç¬¦")

        print(f"\nğŸ“Š å„ç»´åº¦å¾—åˆ†:")
        scores = best.get('scores', {})
        for dim, score in scores.items():
            bar = "â–ˆ" * int(score / 5) + "â–‘" * (20 - int(score / 5))
            print(f"   {dim:20s}: {score:3.0f}/100  {bar}")

        print(f"\nâœ… ä¼˜ç‚¹ ({len(best.get('strengths', []))}):")
        for i, strength in enumerate(best.get('strengths', []), 1):
            print(f"   {i}. {strength}")

        print(f"\nâš ï¸  ç¼ºç‚¹ ({len(best.get('weaknesses', []))}):")
        for i, weakness in enumerate(best.get('weaknesses', []), 1):
            print(f"   {i}. {weakness}")

        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®® ({len(best.get('improvement_suggestions', []))}):")
        for i, suggestion in enumerate(best.get('improvement_suggestions', []), 1):
            print(f"   {i}. é—®é¢˜: {suggestion.get('issue', 'N/A')}")
            print(f"      å»ºè®®: {suggestion.get('suggestion', 'N/A')}")
        # ===============

    except Exception as e:
        print(f"\nâŒ Phase 4 Error: {e}")
        import traceback; traceback.print_exc()
        return

    # ------------------------------------------------------------------
    # Phase 5: éªŒè¯ä¸äº¤ä»˜ (Verification)
    # ------------------------------------------------------------------
    print("\n\n" + "=" * 80)
    print("Phase 5: äº‹å®å‡†ç¡®æ€§éªŒè¯ä¸æœ€ç»ˆæŠ¥å‘Š")
    print("=" * 80)

    try:
        evidence = {}
        for p in papers:
            if p.get("id"): evidence[p["id"]] = p
            if p.get("url"): evidence[p["url"]] = p
        if not evidence:
            for c in cluster_summaries.values():
                for p in c.get("papers", []):
                    evidence[p["paper_id"]] = p

        print(f"\n[Step 5.1] éªŒè¯æœ€ä½³è‰ç¨¿çš„äº‹å®å‡†ç¡®æ€§...")
        verification = judge.verify_factuality(best['draft'], evidence)
        
        print(f"\nâœ… éªŒè¯å®Œæˆ")

        # === è¯¦ç»†æ‰“å° ===
        print(f"\nğŸ“Š éªŒè¯ç»“æœ:")
        print(f"   æ€»é™ˆè¿°æ•°: {verification.get('total_claims', 'N/A')}")
        print(f"   å·²éªŒè¯æ•°: {verification.get('verified_claims', 'N/A')}")
        print(f"   å‡†ç¡®ç‡: {verification.get('accuracy_rate', 0):.1%}")

        citation_check = verification.get('citation_check', {})
        print(f"\nğŸ“ å¼•ç”¨æ£€æŸ¥:")
        print(f"   æ€»å¼•ç”¨æ•°: {citation_check.get('total_citations', 0)}")
        print(f"   æ— æ•ˆå¼•ç”¨æ•°: {len(citation_check.get('invalid_citations', []))}")
        print(f"   å¼•ç”¨æœ‰æ•ˆç‡: {citation_check.get('citation_validity_rate', 0):.1%}")

        if citation_check.get('invalid_citations'):
            print(f"\nâš ï¸  æ— æ•ˆå¼•ç”¨:")
            for citation in citation_check['invalid_citations']:
                print(f"      - [{citation}]")
        else:
            print(f"\nâœ… æ‰€æœ‰å¼•ç”¨å‡æœ‰æ•ˆ")
        # ===============

        # æ„å»ºæœ€ç»ˆæŠ¥å‘Š
        final_report = {
            "meta": {"run_id": RUN_ID, "timestamp": datetime.now().isoformat()},
            "best_draft_content": best['draft'],
            "evaluation": {
                "score": best['overall_score'],
                "details": best.get("scores"),
                "feedback": {
                    "strengths": best.get("strengths"),
                    "weaknesses": best.get("weaknesses"),
                    "improvements": best.get("improvement_suggestions")
                }
            },
            "verification": verification
        }
        
        save_json(final_report, RUN_DIR, "FINAL_REPORT")
        
        md_path = os.path.join(RUN_DIR, "FINAL_PAPER.md")
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(f"# Literature Review: {args.query or 'Auto Survey'}\n\n")
            f.write(f"> Run ID: {RUN_ID} | Score: {best['overall_score']:.1f}\n\n")
            f.write(best['draft'])
            f.write("\n\n---\n*Verified by Judge Agent*")
        
        print("\n\n" + "=" * 80)
        print("æµ‹è¯•æ‘˜è¦")
        print("=" * 80)
        print(f"\nğŸ¯ ç»¼åˆè¯„ä¼°:")
        print(f"   æ€»åˆ†: {best['overall_score']:.1f}/100")
        print(f"   äº‹å®å‡†ç¡®ç‡: {verification.get('accuracy_rate', 0):.1%}")
        print(f"   å¼•ç”¨æœ‰æ•ˆç‡: {citation_check.get('citation_validity_rate', 0):.1%}")

        print(f"\nğŸ“ è´¨é‡ç­‰çº§:")
        score = best['overall_score']
        if score >= 90: grade = "ä¼˜ç§€ï¼ˆå¯å‘è¡¨ï¼‰"
        elif score >= 80: grade = "è‰¯å¥½ï¼ˆéœ€å°å¹…ä¿®æ”¹ï¼‰"
        elif score >= 70: grade = "åˆæ ¼ï¼ˆéœ€ä¸­ç­‰ä¿®æ”¹ï¼‰"
        elif score >= 60: grade = "å°šå¯ï¼ˆéœ€å¤§å¹…ä¿®æ”¹ï¼‰"
        else: grade = "ä¸åˆæ ¼ï¼ˆéœ€é‡å†™ï¼‰"
        print(f"   ç­‰çº§: {grade}")

        print(f"\nâœ… å®Œæ•´æµ‹è¯•æµç¨‹æˆåŠŸå®Œæˆï¼")
        print(f"   ç»“æœç›®å½•: {RUN_DIR}")

    except Exception as e:
        print(f"\nâŒ Phase 5 Error: {e}")
        import traceback; traceback.print_exc()

if __name__ == "__main__":
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())

    # python main.py --query "Coding Benchmark for LLM in Repo-Level" --max-search-steps 5
    # python main.py --analysis-json "ä½ çš„åˆ†æç»“æœjsonè·¯å¾„"
