"""
å®Œæ•´æµ‹è¯•æµç¨‹ï¼šRetrieval -> Analysis -> Writing -> Judging -> Verification
ç‰ˆæœ¬ï¼šFinal Optimized (Detailed Logs + File Save)
"""

import asyncio
import argparse
import json
import os
import sys
from datetime import datetime
from typing import Dict, Any, List
from dotenv import load_dotenv

from agents.retrieval_agent import RetrievalAgent
from agents.analysis_agent import AnalysisAgent
from writing_judging.writing_agent import WritingAgent, ModelConfig
from writing_judging.judge_agent import JudgeAgent
from pipeline_adapter import analysis_to_cluster_summaries
from core.schema import LLMMessage


# ==================== é…ç½®ä¸ç¯å¢ƒ ====================
load_dotenv()
API_KEY = os.getenv("PARATERA_API_KEY") 
BASE_URL = "https://llmapi.paratera.com/v1/"
MODEL_NAME = "Kimi-K2"

if not API_KEY:
    print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ° API Keyï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶ã€‚")

COMMON_AGENT_ARGS = {
    "model": MODEL_NAME,
    "base_url": BASE_URL,
    "api_key": API_KEY
}

WRITER_CONFIG = ModelConfig(
    name=MODEL_NAME, api_key=API_KEY, base_url=BASE_URL, temperature=0.7, max_tokens=8192
)

JUDGE_CONFIG = ModelConfig(
    name="GLM-4.6", api_key=API_KEY, base_url=BASE_URL, temperature=0.2, max_tokens=4096
)

# ==================== æ—¥å¿—ä¸ç›®å½•å·¥å…· ====================

class DualLogger(object):
    """åŒæ—¶å°†è¾“å‡ºå†™å…¥ç»ˆç«¯å’Œæ–‡ä»¶"""
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.log = open(filename, "a", encoding='utf-8')

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush() # ç¡®ä¿å®æ—¶å†™å…¥

    def flush(self):
        self.terminal.flush()
        self.log.flush()

def get_timestamp():
    return datetime.now().strftime("%Y%m%d_%H%M%S")

def create_run_directory(run_id: str):
    base_dir = os.path.join("output", run_id)
    traj_dir = os.path.join(base_dir, "trajectory")
    os.makedirs(traj_dir, exist_ok=True)
    return base_dir, traj_dir

def serialize_history(history: List[Any]) -> List[Dict]:
    serialized = []
    for msg in history:
        if hasattr(msg, 'role'):
            m_dict = {"role": msg.role, "content": msg.content}
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                m_dict["tool_calls"] = [{"name": tc.name, "arguments": tc.arguments} for tc in msg.tool_calls]
            if hasattr(msg, 'tool_call_id') and msg.tool_call_id:
                m_dict["tool_call_id"] = msg.tool_call_id
                m_dict["name"] = msg.name
            serialized.append(m_dict)
        else:
            serialized.append(msg if isinstance(msg, dict) else str(msg))
    return serialized

def save_json(data: Any, folder: str, filename: str):
    if not filename.endswith(".json"):
        filename += ".json"
    path = os.path.join(folder, filename)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return path

def parse_args():
    parser = argparse.ArgumentParser()
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--query", type=str, help="ä»å¤´å¼€å§‹ï¼šæŒ‡å®šç ”ç©¶ä¸»é¢˜")
    group.add_argument("--analysis-json", type=str, help="ä»ä¸­é—´å¼€å§‹ï¼šæŒ‡å®š analysis ç»“æœæ–‡ä»¶")
    parser.add_argument("--max-search-steps", type=int, default=3)
    return parser.parse_args()

# ==================== ä¸»æµç¨‹ ====================

async def main():
    args = parse_args()
    
    # 1. ç›®å½•åˆå§‹åŒ–
    RUN_ID = get_timestamp()
    RUN_DIR, TRAJ_DIR = create_run_directory(RUN_ID)

    # 2. ã€å…³é”®ã€‘é‡å®šå‘ print åˆ°æ—¥å¿—æ–‡ä»¶
    log_file_path = os.path.join(RUN_DIR, "execution_log.txt")
    sys.stdout = DualLogger(log_file_path)
    
    print("=" * 80)
    print(f"ğŸš€ å…¨è‡ªåŠ¨æ–‡çŒ®ç»¼è¿°æµæ°´çº¿å¯åŠ¨")
    print(f"ğŸ“ æœ¬æ¬¡è¿è¡Œç›®å½•: {RUN_DIR}")
    print(f"ğŸ“ å®Œæ•´æ—¥å¿—æ–‡ä»¶: {log_file_path}")
    print("=" * 80)

    analysis_result = {}
    cluster_summaries = {}
    papers = []

    # ------------------------------------------------------------------
    # Phase 1 & 2: æ•°æ®è·å– (Retrieval + Analysis)
    # ------------------------------------------------------------------
    if args.query:
        # >>> Phase 1: Retrieval <<<
        print("\n" + "=" * 80)
        print("Phase 1: Retrieval Agent (æ–‡çŒ®æ£€ç´¢)")
        print("=" * 80)
        
        retriever = RetrievalAgent(**COMMON_AGENT_ARGS)
        papers = await retriever.run(args.query, max_steps=args.max_search_steps)
        
        save_json(serialize_history(retriever.history), TRAJ_DIR, "retrieval_traj")
        
        if not papers:
            print("âŒ æ£€ç´¢å¤±è´¥ï¼Œæœªæ‰¾åˆ°è®ºæ–‡ã€‚")
            return
        save_json(papers, RUN_DIR, "1_retrieval_papers")
        print(f"âœ… æ£€ç´¢å®Œæˆï¼Œè·å– {len(papers)} ç¯‡è®ºæ–‡ã€‚")

        # >>> Phase 2: Analysis <<<
        print("\n" + "=" * 80)
        print("Phase 2: Analysis Agent (æ·±åº¦åˆ†æ)")
        print("=" * 80)
        
        analyzer = AnalysisAgent(datas=papers, **COMMON_AGENT_ARGS)
        analysis_result = await analyzer.run(args.query)
        
        save_json(serialize_history(analyzer.history), TRAJ_DIR, "analysis_traj")
        
        if not analysis_result:
            print("âŒ åˆ†æå¤±è´¥ã€‚")
            return
        save_json(analysis_result, RUN_DIR, "2_analysis_result")
        print(f"âœ… åˆ†æå®Œæˆï¼Œç”Ÿæˆ {len(analysis_result.get('clusters', []))} ä¸ªç ”ç©¶èšç±»ã€‚")
        
        print("\nğŸ”„ Adapting data format...")
        cluster_summaries = analysis_to_cluster_summaries(analysis_result, papers)
        
    else:
        print(f"\nğŸ“‚ Loading analysis from {args.analysis_json}")
        with open(args.analysis_json, "r", encoding="utf-8") as f:
            analysis_result = json.load(f)
        papers = analysis_result.get("datas", [])
        cluster_summaries = analysis_to_cluster_summaries(analysis_result)

    if not cluster_summaries:
        print("âŒ æ— æ³•è·å– Cluster Summariesï¼Œæµç¨‹ç»ˆæ­¢ã€‚")
        return
    
    save_json(cluster_summaries, RUN_DIR, "3_adapter_input")

    # ------------------------------------------------------------------
    # Phase 3: å†™ä½œ (Writing)
    # ------------------------------------------------------------------
    print("\n" + "=" * 80)
    print("Phase 3: ä½¿ç”¨ Writing Agent ç”Ÿæˆè‰ç¨¿")
    print("=" * 80)

    try:
        writer = WritingAgent(WRITER_CONFIG, style="narrative")
        print("\nâœ“ Writing Agent åˆå§‹åŒ–æˆåŠŸ")
        
        print("\n[Step 3.1] ç”Ÿæˆ 3 ä¸ªå€™é€‰è‰ç¨¿ (å¼‚æ­¥å¹¶è¡Œ)...")
        candidates = await writer.generate_multiple_candidates_async(
            cluster_summaries=cluster_summaries,
            num_candidates=3
        )
        
        if not candidates:
            print("âŒ å†™ä½œå¤±è´¥ã€‚")
            return
        save_json(candidates, RUN_DIR, "4_writing_candidates")
        
        print(f"\nâœ… æˆåŠŸç”Ÿæˆ {len(candidates)} ä¸ªå€™é€‰è‰ç¨¿")

        # === è¯¦ç»†æ‰“å° ===
        print("\nå€™é€‰è‰ç¨¿æ‘˜è¦:")
        for i, candidate in enumerate(candidates, 1):
            content_preview = candidate['content'][:150].replace('\n', ' ')
            print(f"\n  è‰ç¨¿ {i}:")
            print(f"    æ¸©åº¦: {candidate.get('temperature', 'N/A'):.2f}")
            print(f"    å¼•ç”¨æ•°: {len(candidate.get('citations', []))}")
            print(f"    é•¿åº¦: {len(candidate.get('content', ''))} å­—ç¬¦")
            print(f"    é¢„è§ˆ: {content_preview}...")
        # ===============

    except Exception as e:
        print(f"\nâŒ Phase 3 Error: {e}")
        import traceback; traceback.print_exc()
        return

    # ------------------------------------------------------------------
    # Phase 4: è¯„å®¡ä¸æ‹©ä¼˜ (Judging)
    # ------------------------------------------------------------------
    print("\n\n" + "=" * 80)
    print("Phase 4: ä½¿ç”¨ Judge Agent è¯„ä¼°è‰ç¨¿")
    print("=" * 80)

    try:
        judge = JudgeAgent(JUDGE_CONFIG)
        print("\nâœ“ Judge Agent åˆå§‹åŒ–æˆåŠŸ")
        
        reference_material = {
            "task": args.query or "Analysis",
            "clusters": cluster_summaries
        }
        draft_texts = [c['content'] for c in candidates]
        
        print("\n[Step 4.1] å¯¹æ‰€æœ‰è‰ç¨¿è¿›è¡Œè¯„åˆ†å’Œæ’åº...")
        ranked_results = await judge.rank_drafts_async(draft_texts, reference=reference_material)
        
        # æ¢å¤å…ƒæ•°æ®
        for rank_item in ranked_results:
            idx = rank_item['draft_id']
            if idx < len(candidates):
                rank_item['meta'] = {
                    "temp": candidates[idx].get("temperature"),
                    "citations": len(candidates[idx].get("citations", []))
                }

        best = ranked_results[0]
        
        save_json(ranked_results, RUN_DIR, "5_judging_result")
        
        print(f"\nâœ… è¯„ä¼°å®Œæˆ")

        # === è¯¦ç»†æ‰“å° ===
        print("\næ’åºç»“æœ:")
        print("-" * 80)
        print(f"{'æ’å':<6} {'è‰ç¨¿ID':<10} {'æ€»åˆ†':<10} {'è¦†ç›–åº¦':<10} {'å‡†ç¡®æ€§':<10} {'è¿è´¯æ€§':<10}")
        print("-" * 80)

        for i, result in enumerate(ranked_results, 1):
            scores = result.get('scores', {})
            print(f"{i:<6} "
                  f"{result['draft_id']:<10} "
                  f"{result['overall_score']:<10.1f} "
                  f"{scores.get('coverage', 0):<10.1f} "
                  f"{scores.get('factuality', 0):<10.1f} "
                  f"{scores.get('coherence', 0):<10.1f}")
        
        # æœ€ä½³è‰ç¨¿è¯¦æƒ…
        print("\n\n" + "=" * 80)
        print("Phase 4.5: æœ€ä½³è‰ç¨¿è¯¦æƒ…")
        print("=" * 80)
        
        print(f"\nğŸ† æœ€ä½³è‰ç¨¿ï¼šè‰ç¨¿ {best['draft_id']}")
        print(f"   æ€»åˆ†: {best['overall_score']:.1f}/100")
        print(f"   é•¿åº¦: {best.get('draft_length', 0)} å­—ç¬¦")

        print(f"\nğŸ“Š å„ç»´åº¦å¾—åˆ†:")
        scores = best.get('scores', {})
        for dim, score in scores.items():
            bar = "â–ˆ" * int(score / 5) + "â–‘" * (20 - int(score / 5))
            print(f"   {dim:20s}: {score:3.0f}/100  {bar}")

        print(f"\nâœ… ä¼˜ç‚¹ ({len(best.get('strengths', []))}):")
        for i, strength in enumerate(best.get('strengths', []), 1):
            print(f"   {i}. {strength}")

        print(f"\nâš ï¸  ç¼ºç‚¹ ({len(best.get('weaknesses', []))}):")
        for i, weakness in enumerate(best.get('weaknesses', []), 1):
            print(f"   {i}. {weakness}")

        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®® ({len(best.get('improvement_suggestions', []))}):")
        for i, suggestion in enumerate(best.get('improvement_suggestions', []), 1):
            print(f"   {i}. é—®é¢˜: {suggestion.get('issue', 'N/A')}")
            print(f"      å»ºè®®: {suggestion.get('suggestion', 'N/A')}")
        # ===============

    except Exception as e:
        print(f"\nâŒ Phase 4 Error: {e}")
        import traceback; traceback.print_exc()
        return

    # ------------------------------------------------------------------
    # Phase 5: éªŒè¯ä¸äº¤ä»˜ (Verification)
    # ------------------------------------------------------------------
    print("\n\n" + "=" * 80)
    print("Phase 5: äº‹å®å‡†ç¡®æ€§éªŒè¯ä¸æœ€ç»ˆæŠ¥å‘Š")
    print("=" * 80)

    try:
        evidence = {}
        for p in papers:
            if p.get("id"): evidence[p["id"]] = p
            if p.get("url"): evidence[p["url"]] = p
        if not evidence:
            for c in cluster_summaries.values():
                for p in c.get("papers", []):
                    evidence[p["paper_id"]] = p

        print(f"\n[Step 5.1] éªŒè¯æœ€ä½³è‰ç¨¿çš„äº‹å®å‡†ç¡®æ€§...")
        verification = judge.verify_factuality(best['draft'], evidence)
        
        print(f"\nâœ… éªŒè¯å®Œæˆ")

        # === è¯¦ç»†æ‰“å° ===
        print(f"\nğŸ“Š éªŒè¯ç»“æœ:")
        print(f"   æ€»é™ˆè¿°æ•°: {verification.get('total_claims', 'N/A')}")
        print(f"   å·²éªŒè¯æ•°: {verification.get('verified_claims', 'N/A')}")
        print(f"   å‡†ç¡®ç‡: {verification.get('accuracy_rate', 0):.1%}")

        citation_check = verification.get('citation_check', {})
        print(f"\nğŸ“ å¼•ç”¨æ£€æŸ¥:")
        print(f"   æ€»å¼•ç”¨æ•°: {citation_check.get('total_citations', 0)}")
        print(f"   æ— æ•ˆå¼•ç”¨æ•°: {len(citation_check.get('invalid_citations', []))}")
        print(f"   å¼•ç”¨æœ‰æ•ˆç‡: {citation_check.get('citation_validity_rate', 0):.1%}")

        if citation_check.get('invalid_citations'):
            print(f"\nâš ï¸  æ— æ•ˆå¼•ç”¨:")
            for citation in citation_check['invalid_citations']:
                print(f"      - [{citation}]")
        else:
            print(f"\nâœ… æ‰€æœ‰å¼•ç”¨å‡æœ‰æ•ˆ")
        # ===============

        # æ„å»ºæœ€ç»ˆæŠ¥å‘Š
        final_report = {
            "meta": {"run_id": RUN_ID, "timestamp": datetime.now().isoformat()},
            "best_draft_content": best['draft'],
            "evaluation": {
                "score": best['overall_score'],
                "details": best.get("scores"),
                "feedback": {
                    "strengths": best.get("strengths"),
                    "weaknesses": best.get("weaknesses"),
                    "improvements": best.get("improvement_suggestions")
                }
            },
            "verification": verification
        }
        
        save_json(final_report, RUN_DIR, "FINAL_REPORT")
        
        md_path = os.path.join(RUN_DIR, "FINAL_PAPER.md")
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(f"# Literature Review: {args.query or 'Auto Survey'}\n\n")
            f.write(f"> Run ID: {RUN_ID} | Score: {best['overall_score']:.1f}\n\n")
            f.write(best['draft'])
            f.write("\n\n---\n*Verified by Judge Agent*")
        
        print("\n\n" + "=" * 80)
        print("æµ‹è¯•æ‘˜è¦")
        print("=" * 80)
        print(f"\nğŸ¯ ç»¼åˆè¯„ä¼°:")
        print(f"   æ€»åˆ†: {best['overall_score']:.1f}/100")
        print(f"   äº‹å®å‡†ç¡®ç‡: {verification.get('accuracy_rate', 0):.1%}")
        print(f"   å¼•ç”¨æœ‰æ•ˆç‡: {citation_check.get('citation_validity_rate', 0):.1%}")

        print(f"\nğŸ“ è´¨é‡ç­‰çº§:")
        score = best['overall_score']
        if score >= 90: grade = "ä¼˜ç§€ï¼ˆå¯å‘è¡¨ï¼‰"
        elif score >= 80: grade = "è‰¯å¥½ï¼ˆéœ€å°å¹…ä¿®æ”¹ï¼‰"
        elif score >= 70: grade = "åˆæ ¼ï¼ˆéœ€ä¸­ç­‰ä¿®æ”¹ï¼‰"
        elif score >= 60: grade = "å°šå¯ï¼ˆéœ€å¤§å¹…ä¿®æ”¹ï¼‰"
        else: grade = "ä¸åˆæ ¼ï¼ˆéœ€é‡å†™ï¼‰"
        print(f"   ç­‰çº§: {grade}")

        print(f"\nâœ… å®Œæ•´æµ‹è¯•æµç¨‹æˆåŠŸå®Œæˆï¼")
        print(f"   ç»“æœç›®å½•: {RUN_DIR}")

    except Exception as e:
        print(f"\nâŒ Phase 5 Error: {e}")
        import traceback; traceback.print_exc()

if __name__ == "__main__":
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())

    # python main.py --query "Coding Benchmark for LLM in Repo-Level" --max-search-steps 5
    # python main.py --analysis-json "ä½ çš„åˆ†æç»“æœjsonè·¯å¾„"
