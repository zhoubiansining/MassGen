"""
å®Œæ•´æµç¨‹ï¼šæŒ‰å›ºå®šæ¸©åº¦æ‰¹é‡ç”Ÿæˆå€™é€‰è‰ç¨¿ + SurveyGen è‡ªåŠ¨æŒ‡æ ‡ + å¤š LLM è¯„åˆ†èåˆï¼Œè¾“å‡ºæœ€ä½³è‰ç¨¿ã€‚
"""

import argparse
import json
import os
from typing import Dict, List, Optional

from writing_agent import WritingAgent, ModelConfig
from judge_agent import JudgeAgent
from pipeline_adapter import analysis_to_cluster_summaries


CONFIG = ModelConfig(
    name="Kimi-K2",
    api_key="sk-aRG9iu2Hy9--oPxrG-5faA",
    base_url="https://llmapi.paratera.com/v1/",
    temperature=0.5,
    max_tokens=4096,
)


DEFAULT_TEST_DATA = {
    "cluster_1": {
        "topic": "Transformer æ¶æ„ä¸æ³¨æ„åŠ›æœºåˆ¶",
        "summary": "æœ¬ä¸»é¢˜æ¶µç›–äº† Transformer æ¶æ„çš„æå‡ºåŠå…¶æ³¨æ„åŠ›æœºåˆ¶çš„æ”¹è¿›æ–¹æ³•ã€‚",
        "papers": [
            {
                "paper_id": "paper_001",
                "title": "Attention is All You Need",
                "authors": ["Vaswani, A.", "Shazeer, N."],
                "year": 2017,
                "key_contribution": "æå‡ºäº†åŸå§‹ Transformer æ¶æ„ï¼ŒåŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶",
            },
            {
                "paper_id": "paper_002",
                "title": "BERT: Pre-training of Deep Bidirectional Transformers",
                "authors": ["Devlin, J.", "Chang, M. W."],
                "year": 2019,
                "key_contribution": "åŒå‘é¢„è®­ç»ƒæ–¹æ³•ï¼ŒMasked Language Model",
            },
            {
                "paper_id": "paper_003",
                "title": "GPT-3: Language Models are Few-Shot Learners",
                "authors": ["Brown, T.", "Mann, B."],
                "year": 2020,
                "key_contribution": "å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›",
            },
        ],
    }
}


def parse_args():
    parser = argparse.ArgumentParser(description="å†™ä½œ + è¯„åˆ†èåˆæµæ°´çº¿")
    parser.add_argument("--analysis-json", dest="analysis_json", help="å¯é€‰ï¼Œå‰åº analysis è¾“å‡º JSON è·¯å¾„")
    return parser.parse_args()


def load_cluster_summaries(analysis_json: Optional[str]):
    if analysis_json:
        if not os.path.exists(analysis_json):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ° analysis_json æ–‡ä»¶: {analysis_json}")
        with open(analysis_json, "r", encoding="utf-8") as f:
            analysis_result = json.load(f)
        print(f"\nğŸ“¥ å·²è¯»å– {analysis_json}ï¼Œè½¬æ¢ä¸º cluster_summaries")
        return analysis_to_cluster_summaries(analysis_result)
    print("\nğŸ“¥ æœªæä¾› analysis_jsonï¼Œä½¿ç”¨å†…ç½®ç¤ºä¾‹æ•°æ®")
    return DEFAULT_TEST_DATA


def compute_auto_metrics(candidate: Dict, cluster_summaries: Dict) -> Dict:
    """åŸºäº SurveyGen æŒ‡æ ‡ï¼šç›®å‰å®ç°å¼•æ–‡è´¨é‡ï¼ˆç²¾åº¦/å¬å›/F1/å‡†ç¡®ç‡ï¼‰ï¼Œå†…å®¹ä¸ç»“æ„æŒ‡æ ‡ç•™ç©ºå¾…å¯¹æ¥ã€‚"""
    human_refs = {
        paper.get("paper_id")
        for cluster in cluster_summaries.values()
        for paper in cluster.get("papers", [])
        if paper.get("paper_id")
    }
    pred_refs = set(candidate.get("citations", []))

    matches = len(pred_refs & human_refs)
    prec = matches / len(pred_refs) if pred_refs else 0.0
    rec = matches / len(human_refs) if human_refs else 0.0
    f1 = 2 * prec * rec / (prec + rec) if prec + rec > 0 else 0.0

    return {
        "citation": {
            "precision": prec,
            "recall": rec,
            "f1": f1,
            "accuracy": prec,
        },
        "content": {},
        "structure": {},
    }


def main():
    args = parse_args()
    cluster_summaries = load_cluster_summaries(args.analysis_json)

    # 1) ç”Ÿæˆå€™é€‰è‰ç¨¿ï¼šæ¸©åº¦ 0.3/0.4/0.5 å„ 3 ç¯‡ï¼Œå…± 9 ç¯‡
    writer = WritingAgent(CONFIG, style="narrative")
    temps = [0.3, 0.4, 0.5]
    candidates = writer.generate_candidates_by_temps(
        cluster_summaries=cluster_summaries,
        temps=temps,
        per_temp=3,
    )

    print(f"ç”Ÿæˆå®Œæˆï¼šå…± {len(candidates)} ç¯‡å€™é€‰è‰ç¨¿")

    # 2) è¯„åˆ†èåˆï¼šSurveyGen è‡ªåŠ¨æŒ‡æ ‡ + å¤š LLM ä¸»è§‚è¯„åˆ†
    model_configs: List[ModelConfig] = [CONFIG]  # å¯åœ¨æ­¤å¤„è¿½åŠ æ›´å¤šæ¨¡å‹
    ranked = []
    for cand in candidates:
        auto_metrics = compute_auto_metrics(cand, cluster_summaries)
        report = JudgeAgent.multi_model_vote_with_auto_metrics(
            draft=cand["content"],
            model_configs=model_configs,
            reference=cluster_summaries,
            auto_metrics=auto_metrics,
            auto_weight=0.5,
        )
        ranked.append({
            "candidate_id": cand.get("candidate_id"),
            "temperature": cand.get("temperature"),
            "citations": cand.get("citations", []),
            "final_score": report["final_score"],
            "llm_average": report.get("llm_average"),
            "auto_metrics": auto_metrics,
            "auto_score": report.get("auto_evaluation", {}).get("overall_score"),
            "details": report,
            "content": cand["content"],
        })

    ranked.sort(key=lambda x: x["final_score"], reverse=True)

    # 3) è¾“å‡ºç»“æœ
    print("\næ’åºç»“æœ (å‰ 5)ï¼š")
    for i, item in enumerate(ranked[:5], 1):
        print(
            f"{i}. è‰ç¨¿ID={item['candidate_id']}, temp={item['temperature']}, "
            f"æœ€ç»ˆåˆ†={item['final_score']:.2f}, LLMå‡åˆ†={item['llm_average']:.2f}, è‡ªåŠ¨åˆ†={item['auto_score'] or 0:.2f}"
        )

    best = ranked[0]
    print("\nğŸ† æœ€ä½³è‰ç¨¿æ‘˜è¦:")
    print(f"- è‰ç¨¿ID: {best['candidate_id']}")
    print(f"- æ¸©åº¦: {best['temperature']}")
    print(f"- æœ€ç»ˆåˆ†: {best['final_score']:.2f}")
    print(f"- è‡ªåŠ¨å¼•æ–‡æŒ‡æ ‡: P={best['auto_metrics']['citation']['precision']:.2f}, "
          f"R={best['auto_metrics']['citation']['recall']:.2f}, F1={best['auto_metrics']['citation']['f1']:.2f}")
    print("\nå‰ 500 å­—é¢„è§ˆ:\n" + best["content"][:500])


if __name__ == "__main__":
    main()
